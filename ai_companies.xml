<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Daily Digest - Ai Companies</title><link>https://your-github-pages-url/ai_companies.xml</link><description>Latest AI news and updates from ai companies sources</description><atom:link href="https://your-github-pages-url/ai_companies.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 04 Mar 2025 06:26:17 +0000</lastBuildDate><item><title>Get coding help from Gemini Code Assist — now for free</title><link>https://blog.google/technology/developers/gemini-code-assist-free/</link><description>Dark-themed code editor with highlighted lines. Two inset panels show illustrated people working on laptops, one with a hijab. A sparkling icon is centered below.</description><content:encoded>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini-Code-Assist_v4_1.max-600x600.format-webp.webp"&gt;Announcing a free version of Gemini Code Assist, powered by Gemini 2.0, and Gemini Code Review in GitHub. </content:encoded><pubDate>Tue, 25 Feb 2025 11:00:00 +0000</pubDate></item><item><title>Sign up for this year’s Gen AI Intensive live course from Google and Kaggle.</title><link>https://blog.google/feed/kaggle-genai-intensive-course-2025/</link><description>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/16x9_GenAI-event-2025---Illustr.max-600x600.format-webp.webp" /&gt;In 2024, more than 140,000 people participated in Google and Kaggle’s Gen AI Intensive live course. Our course is returning this year, with updated content, new speakers…</description><content:encoded> </content:encoded><pubDate>Tue, 25 Feb 2025 14:00:00 +0000</pubDate></item><item><title>Sign up for this year’s Gen AI Intensive live course from Google and Kaggle.</title><link>https://blog.google/feed/kaggle-genai-intensive-course-2025/</link><description>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/16x9_GenAI-event-2025---Illustr.max-600x600.format-webp.webp" /&gt;In 2024, more than 140,000 people participated in Google and Kaggle’s Gen AI Intensive live course. Our course is returning this year, with updated content, new speakers…</description><content:encoded> </content:encoded><pubDate>Tue, 25 Feb 2025 14:00:00 +0000</pubDate></item><item><title>Read our first Health Impact Report.</title><link>https://blog.google/feed/health-impact-report-2025/</link><description>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GoogleHealth_SS.max-600x600.format-webp.webp" /&gt;Technology like AI is changing the ways we prevent, diagnose and treat diseases to make healthcare more accessible and human, putting people at the heart of innovation.T…</description><content:encoded> </content:encoded><pubDate>Tue, 25 Feb 2025 18:00:00 +0000</pubDate></item><item><title>Start building with Gemini 2.0 Flash and Flash-Lite</title><link>https://deepmind.google/discover/blog/start-building-with-gemini-20-flash-and-flash-lite/</link><description>Gemini 2.0 Flash-Lite is now generally available in the Gemini API for production use in Google AI Studio and for enterprise customers on Vertex AI</description><content:encoded>Gemini 2.0 Flash-Lite is now generally available in the Gemini API for production use in Google AI Studio and for enterprise customers on Vertex AI</content:encoded><pubDate>Tue, 25 Feb 2025 18:02:12 +0000</pubDate></item><item><title>Start building with Gemini 2.0 Flash and Flash-Lite</title><link>https://deepmind.google/discover/blog/start-building-with-gemini-20-flash-and-flash-lite/</link><description>Gemini 2.0 Flash-Lite is now generally available in the Gemini API for production use in Google AI Studio and for enterprise customers on Vertex AI</description><content:encoded>Gemini 2.0 Flash-Lite is now generally available in the Gemini API for production use in Google AI Studio and for enterprise customers on Vertex AI</content:encoded><pubDate>Tue, 25 Feb 2025 18:02:12 +0000</pubDate></item><item><title>Quickly Start Evaluating LLMs With OpenEvals</title><link>https://blog.langchain.dev/evaluating-llms-with-openevals/</link><description>&lt;p&gt;Evaluations (evals) are important for bringing reliable LLM powered applications or agents to production, but it can be hard to know where to start when building evaluations from scratch. Our new packages&amp;#x2014;&lt;a href="https://github.com/langchain-ai/openevals?ref=blog.langchain.dev"&gt;openevals&lt;/a&gt; and &lt;a href="https://github.com/langchain-ai/agentevals?ref=blog.langchain.dev"&gt;agentevals&lt;/a&gt;&amp;#x2014;provide a set of evaluators and a common framework that you can easily&lt;/p&gt;</description><content:encoded>&lt;img alt="Quickly Start Evaluating LLMs With OpenEvals" src="https://blog.langchain.dev/content/images/2025/02/Open-Evals---BLOG.png" /&gt;&lt;p&gt;Evaluations (evals) are important for bringing reliable LLM powered applications or agents to production, but it can be hard to know where to start when building evaluations from scratch. Our new packages&amp;#x2014;&lt;a href="https://github.com/langchain-ai/openevals?ref=blog.langchain.dev"&gt;openevals&lt;/a&gt; and &lt;a href="https://github.com/langchain-ai/agentevals?ref=blog.langchain.dev"&gt;agentevals&lt;/a&gt;&amp;#x2014;provide a set of evaluators and a common framework that you can easily get started with.&lt;/p&gt;&lt;h2 id="what-are-evals"&gt;What are evals?&lt;/h2&gt;&lt;p&gt;Evals provide systematic ways to judge LLM output quality based on criteria that&amp;apos;s important for your application. There are two components of evals: the &lt;strong&gt;data that you&amp;#x2019;re evaluating over&lt;/strong&gt; and the &lt;strong&gt;metric that you&amp;#x2019;re evaluating on&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;The quality and diversity of the data you&amp;#x2019;re evaluating over directly influences how well the evaluation reflects real-world usage. Before you create an evaluation, spend some time curating a dataset for your specific use case&amp;#x2014; you only need a handful of high quality data points to get started. Read more about dataset curation &lt;a href="https://docs.smith.langchain.com/evaluation/concepts?ref=blog.langchain.dev#dataset-curation"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The metrics you&amp;apos;re evaluating are also often custom depending on the goals of your application, however, we see common trends in the kinds of evaluations that are used. This is why we built &lt;a href="https://github.com/langchain-ai/openevals?ref=blog.langchain.dev"&gt;openevals&lt;/a&gt; and &lt;a href="https://github.com/langchain-ai/agentevals?ref=blog.langchain.dev"&gt;agentevals&lt;/a&gt; &amp;#x2014; to share prebuilt solutions that show common evaluation trends and best practices to help you get started.&lt;/p&gt;&lt;h2 id="common-evaluation-types-and-best-practices"&gt;Common Evaluation Types and Best Practices&lt;/h2&gt;&lt;p&gt;There are many types of evaluations, but to start, we&amp;#x2019;ve focused on releasing eval techniques that we&amp;#x2019;ve seen are the most commonly used and practically useful. We&amp;#x2019;re approaching this in two ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Making broadly applicable evaluators easy to customize:&lt;/strong&gt; LLM-as-a-judge evals are the most broadly applicable evaluators. &lt;code&gt;openevals&lt;/code&gt; makes it easy to take pre-built examples and customize them specific to your use case.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Making evaluators for specific use cases&lt;/strong&gt;: There are an endless number of use cases, but we&amp;#x2019;ll be building off-the-shelf evaluation for the most common ones. To start, &lt;code&gt;openevals&lt;/code&gt; and &lt;code&gt;angentevals&lt;/code&gt; cover cases in an application where you&amp;#x2019;re extracting structured content from documents, managing tool calls and agent trajectories. We plan to expand the libraries to include more specific techniques depending on application type (eg. evals specific to RAG applications or multi-agent architectures).&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;LLM-as-a-judge evals&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LLM-as-judge evaluators use LLMs to score your application&amp;apos;s output. These are the most common types of evaluators we see since they&amp;#x2019;re primarily used when evaluating natural language outputs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Use When Evaluating:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Conversational quality of chatbot responses&lt;/li&gt;&lt;li&gt;To test for hallucination in summarization or question-answering systems&lt;/li&gt;&lt;li&gt;Writing quality and coherence&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Importantly, LLM-as-judge evaluations can be reference free, allowing you to judge responses objectively without requiring ground truth answers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How &lt;code&gt;openevals&lt;/code&gt; Helps:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Pre-built starter prompts that you can easily customize&lt;/li&gt;&lt;li&gt;Incorporate few-shot examples to better align with human preferences&lt;/li&gt;&lt;li&gt;Simplifies the process of setting up a scoring schema for consistent evaluation&lt;/li&gt;&lt;li&gt;Generates reasoning comments for why a particular score was given, adding transparency to the evaluation process&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;View examples and get started with LLM-as-a-judge evaluators &lt;a href="https://github.com/langchain-ai/openevals?tab=readme-ov-file&amp;amp;ref=blog.langchain.dev#llm-as-judge"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;Structured Data Evals&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Many LLM applications involve extracting structured output from documents or generating structured output for tool calling. For these cases, it&amp;#x2019;s important that the model&amp;#x2019;s output conforms to a predefined format.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Use When Evaluating:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Structured information extracted from PDFs, images or other documents&lt;/li&gt;&lt;li&gt;Consistently formatted JSON or other structured outputs&lt;/li&gt;&lt;li&gt;Validating parameters for tool calls (eg. API calls)&lt;/li&gt;&lt;li&gt;Ensuring outputs match specific formats or fall within a category&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;How &lt;code&gt;openevals&lt;/code&gt; Helps:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;openevals&lt;/code&gt; provides the ability configure exact match or use LLM-as-a-judge to validate the structured output&lt;/li&gt;&lt;li&gt;Optionally, aggregate scores across feedback keys for a high level view of evaluator performance&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;View examples and get started with structured data evaluators &lt;a href="https://github.com/langchain-ai/openevals?tab=readme-ov-file&amp;amp;ref=blog.langchain.dev#extraction-and-tool-calls"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;Agent evaluations: Trajectory evaluations&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When building an agent, you&amp;#x2019;re often interested in more than just the final output&amp;#x2014;you want to understand how the agent reached that result. Trajectory evaluation assesses the sequence of actions an agent takes to complete a task.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Use When Evaluating:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Tools or sequence of tools selection&lt;/li&gt;&lt;li&gt;The trajectory of a &lt;a href="https://langchain-ai.github.io/langgraph/?ref=blog.langchain.dev"&gt;LangGraph&lt;/a&gt; application&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;How &lt;code&gt;agentevals&lt;/code&gt; Helps:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/langchain-ai/agentevals?tab=readme-ov-file&amp;amp;ref=blog.langchain.dev#agent-trajectory"&gt;Agent Trajectory&lt;/a&gt; allows you to check that your agent is calling the right tools (optionally with a strict order) or use LLM-as-a-judge to evaluate the trajectory&lt;/li&gt;&lt;li&gt;If you&amp;#x2019;re using &lt;a href="https://langchain-ai.github.io/langgraph/?ref=blog.langchain.dev"&gt;LangGraph&lt;/a&gt; you can use &lt;a href="https://github.com/langchain-ai/agentevals?tab=readme-ov-file&amp;amp;ref=blog.langchain.dev#graph-trajectory"&gt;Graph Trajectory&lt;/a&gt; to ensure that your agent is calling the right nodes&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;View examples and get started with agent evaluations &lt;a href="https://github.com/langchain-ai/agentevals?ref=blog.langchain.dev"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3 id="track-results-over-time-with-langsmith"&gt;Track results over time with LangSmith&lt;/h3&gt;&lt;p&gt;For tracking evaluations over time and sharing them with a team, we recommend logging results to&amp;#xa0;&lt;a href="https://smith.langchain.com/?ref=blog.langchain.dev"&gt;LangSmith&lt;/a&gt;. Companies like &lt;a href="https://www.elastic.co/blog/elastic-security-generative-ai-features?ref=blog.langchain.dev"&gt;Elastic&lt;/a&gt;, &lt;a href="https://blog.langchain.dev/customers-klarna/"&gt;Klarna&lt;/a&gt;, and &lt;a href="https://blog.langchain.dev/customers-podium/" rel="noreferrer"&gt;Podium&lt;/a&gt; use LangSmith to evaluate their GenAI applications.&lt;/p&gt;&lt;p&gt;LangSmith includes tracing, evaluation, and experimentation tools to help you build production-grade LLM applications. Visit our guides on how to integrate &lt;a href="https://github.com/langchain-ai/openevals?tab=readme-ov-file&amp;amp;ref=blog.langchain.dev#langsmith-integration"&gt;openevals&lt;/a&gt; or &lt;a href="https://github.com/langchain-ai/agentevals?tab=readme-ov-file&amp;amp;ref=blog.langchain.dev#langsmith-integration"&gt;agentevals&lt;/a&gt; with LangSmith.&lt;/p&gt;&lt;h3 id="more-coming-soon"&gt;More coming soon!&lt;/h3&gt;&lt;p&gt;This is just the beginning of our ongoing effort to codify best practices for evaluating different types of applications. In the coming weeks, we&amp;#x2019;ll be adding more specific evaluators for common use cases, and more evaluators for testing agents.&lt;/p&gt;&lt;p&gt;Have ideas for evaluators you&amp;apos;d like to see? Open an issue on our GitHub repositories (&lt;a href="https://github.com/langchain-ai/openevals?ref=blog.langchain.dev"&gt;openevals&lt;/a&gt; and &lt;a href="https://github.com/langchain-ai/agentevals?ref=blog.langchain.dev"&gt;agentevals&lt;/a&gt;). If you&amp;apos;ve developed evaluators that have worked well for your applications, we welcome pull requests to share them with the community.&lt;/p&gt; </content:encoded><pubDate>Wed, 26 Feb 2025 18:35:25 +0000</pubDate></item><item><title>How MUFG Bank increased sales efficiency by 10x with LangChain</title><link>https://blog.langchain.dev/customers-mufgbank/</link><description>See how MUFG Bank used LangChain to streamline corporate sales research, cutting data analysis time from hours to minutes and boosting efficiency 10x.</description><content:encoded>&lt;img alt="How MUFG Bank increased sales efficiency by 10x with LangChain" src="https://blog.langchain.dev/content/images/2025/02/MUFG-Case-study.png" /&gt;&lt;p&gt;MUFG Bank is Japan&amp;#x2019;s largest bank and one of the world&amp;apos;s leading financial institutions. They provide capital market solutions to major corporate clients and promote economic growth around the world.&amp;#xa0;&lt;/p&gt;&lt;h2 id="problem-solving-data-overload-for-corporate-sales"&gt;&lt;strong&gt;Problem: Solving data overload for corporate sales&amp;#xa0;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;In MUFG Bank&amp;apos;s Global Capital Markets Division, the FX &amp;amp; Derivative Sales team faced a key challenge. FX &amp;amp; Derivative Sales team members needed to gather and analyze vast amounts of corporate data in order to create compelling client presentations &amp;#x2013; from 10k reports, to market data, to financial disclosures. This was a time-consuming process and skill-dependent (with junior members often needing additional guidance and assistance), which limited efficiency.&lt;/p&gt;&lt;p&gt;To address these challenges, MUFG&amp;#x2019;s AI/ML team leveraged Generative AI (GenAI) to streamline data digestion and automate the creation of presentation materials. Their goal was to empower sales teams with rapid insights, reducing manual burden and ensuring more effective client interactions.&lt;/p&gt;&lt;h2 id="solution-using-langchain-for-retrieval-and-summarization"&gt;&lt;strong&gt;Solution: Using LangChain for retrieval and summarization&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;To improve the FX &amp;amp; Derivative Sales team&amp;#x2019;s client research process, the MUFG AI/ML team implemented two key steps:&lt;/p&gt;&lt;h3 id="1-data-extraction-summarization"&gt;&lt;strong&gt;1) Data extraction &amp;amp; summarization&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Annual reports often spanned 100-200 pages, with only a fraction containing relevant insights for the sales teams. Using LangChain, MUFG developed a system to extract critical financial data efficiently &amp;#x2013; and they implemented fine-tuned prompt engineering and retrieval-augmented generation (RAG) to surface the most relevant sections for sales teams.&lt;/p&gt;&lt;h3 id="2-automatically-generate-presentations"&gt;&lt;strong&gt;2) Automatically generate presentations&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;The FX &amp;amp; Derivative Sales teams required tailored presentations based on the extracted insights. To ensure the insights were actionable, the AI/ML team implemented few-shot prompting techniques and step-by-step guidance that helped FX &amp;amp; Derivative Sales professionals &amp;#x2013; even those with limited experience &amp;#x2013; quickly analyze financial opportunities and provide structured recommendations.&amp;#xa0;&lt;/p&gt;&lt;p&gt;This enabled sales teams to assess interest rate risks, identify potential FX derivative purchases, and suggest regional currency positioning strategies.&amp;#xa0;&lt;/p&gt;&lt;p&gt;The production RAG application now serves as a knowledge-sharing tool for corporate sales teams, simplifying the search for internal documents and deal-making ideas.&lt;/p&gt;&lt;h2 id="impact-improving-efficiency-10x-in-sales-processes"&gt;&lt;strong&gt;Impact: Improving efficiency 10x in sales processes&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The adoption of LangChain-powered GenAI has yielded substantial improvements for MUFG&amp;#x2019;s corporate sales team. Specifically, the process of analyzing corporate client data and generating presentation materials has been &lt;strong&gt;reduced&lt;/strong&gt; from several hours to just &lt;strong&gt;3-5 minutes&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;Previously, only limited experienced sales personnel could manually generate insightful presentations. With the new system, hundreds of sales professionals can now access the same level of intelligence, leading to a &lt;strong&gt;10x increase&lt;/strong&gt; in the number of corporate clients receiving tailored financial recommendations.&lt;/p&gt;&lt;p&gt;These efficiency gains have also begun converting into tangible business outcomes, with deal execution timelines shortening over the past six months.&lt;/p&gt;&lt;h2 id="behind-the-scenes-how-langchain-enabled-mufg%E2%80%99s-success"&gt;&lt;strong&gt;Behind the scenes: How LangChain enabled MUFG&amp;#x2019;s success&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The MUFG AI/ML team benefited from the LangChain programming library in the following two phases:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;R&amp;amp;D / PoC phase&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The MUFG AI/ML team chose the Python version of LangChain and built a simple chat and RAG app. LangChain is well integrated with Streamlit, allowing them to easily manage conversation history and implement interactive apps. This enabled them to quickly start experiments, gather feedback from the sales, and iterate on improvements. Furthermore, thanks to the Retriever interface, they were able to switch between several specific vector databases and search engines, allowing them to compare and&amp;#xa0;validate the accuracy at a low implementation cost.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Development / Production phase&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The MUFG team switched to the TypeScript version of LangChain for a more sustainable and secure application via Next.js. The interface was nearly identical to the Python version, ensuring a smooth transition. In addition, Runnable Lambda allowed them to dynamically change the content filter and target index on demand and enabled them to invoke it in their custom RAG chain.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-next"&gt;&lt;strong&gt;What&amp;#x2019;s Next&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;MUFG Bank plans to refine its GenAI applications by enhancing its evaluation metrics, exploring graph-based AI architectures or AI agents for complex reasoning tasks, and expanding its RAG-driven retrieval system to incorporate broader financial data sources.&amp;#xa0;&lt;/p&gt;&lt;p&gt;By leveraging LangChain, MUFG continues to advance AI-driven sales intelligence, improving efficiency, scalability, and strategic decision-making for its global clientele.&lt;/p&gt; </content:encoded><pubDate>Thu, 27 Feb 2025 07:29:48 +0000</pubDate></item><item><title>LangGraph 0.3 Release: Prebuilt Agents</title><link>https://blog.langchain.dev/langgraph-0-3-release-prebuilt-agents/</link><description>&lt;p&gt;By Nuno Campos and Vadym Barda&lt;/p&gt;&lt;p&gt;Over the past year, we&amp;#x2019;ve invested heavily in making &lt;a href="https://langchain-ai.github.io/langgraph/?ref=blog.langchain.dev"&gt;LangGraph&lt;/a&gt; the go-to framework for building AI agents. With companies like &lt;a href="https://blog.langchain.dev/customers-replit/"&gt;Replit&lt;/a&gt;, &lt;a href="https://blog.langchain.dev/customers-klarna/"&gt;Klarna&lt;/a&gt;, &lt;a href="https://www.linkedin.com/blog/engineering/ai/practical-text-to-sql-for-data-analytics?ref=blog.langchain.dev"&gt;LinkedIn&lt;/a&gt; and &lt;a href="https://dpe.org/sessions/ty-smith-adam-huda/this-year-in-ubers-ai-driven-developer-productivity-revolution/?ref=blog.langchain.dev"&gt;Uber&lt;/a&gt; choosing to build on top of LangGraph, we have more conviction than ever that we are on&lt;/p&gt;</description><content:encoded>&lt;img alt="LangGraph 0.3 Release: Prebuilt Agents" src="https://blog.langchain.dev/content/images/2025/02/Youtube-and-Blog-Self-Serve-Components--1-.png" /&gt;&lt;p&gt;By Nuno Campos and Vadym Barda&lt;/p&gt;&lt;p&gt;Over the past year, we&amp;#x2019;ve invested heavily in making &lt;a href="https://langchain-ai.github.io/langgraph/?ref=blog.langchain.dev"&gt;LangGraph&lt;/a&gt; the go-to framework for building AI agents. With companies like &lt;a href="https://blog.langchain.dev/customers-replit/"&gt;Replit&lt;/a&gt;, &lt;a href="https://blog.langchain.dev/customers-klarna/"&gt;Klarna&lt;/a&gt;, &lt;a href="https://www.linkedin.com/blog/engineering/ai/practical-text-to-sql-for-data-analytics?ref=blog.langchain.dev"&gt;LinkedIn&lt;/a&gt; and &lt;a href="https://dpe.org/sessions/ty-smith-adam-huda/this-year-in-ubers-ai-driven-developer-productivity-revolution/?ref=blog.langchain.dev"&gt;Uber&lt;/a&gt; choosing to build on top of LangGraph, we have more conviction than ever that we are on the right path.&lt;/p&gt;&lt;p&gt;A core principle of LangGraph is to be as low level as possible. There are no hidden prompts or no enforced &amp;#x201c;&lt;a href="https://blog.langchain.dev/what-is-a-cognitive-architecture/"&gt;cognitive architectures&lt;/a&gt;&amp;#x201d; in LangGraph. This has served to make it production ready and also distinguishes itself from all other frameworks.&lt;/p&gt;&lt;p&gt;At the same, we &lt;strong&gt;do&lt;/strong&gt; see the value in higher level abstractions. They make it easy to get started, easy to try out new cognitive architectures, and provide a nice entrypoint to the field.&lt;/p&gt;&lt;p&gt;Up to this point, we&amp;#x2019;ve had one higher level abstraction and it&amp;#x2019;s lived in the main &lt;code&gt;langgraph&lt;/code&gt; package. It was &lt;a href="https://langchain-ai.github.io/langgraph/how-tos/?ref=blog.langchain.dev#prebuilt-react-agent"&gt;&lt;code&gt;create_react_agent&lt;/code&gt;&lt;/a&gt;, a wrapper for creating a simple tool calling agent. Today, we are splitting that out of &lt;code&gt;langgraph&lt;/code&gt; as part of a 0.3 release, and moving it into &lt;code&gt;langgraph-prebuilt&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;We are also introducing a new set of &lt;a href="https://langchain-ai.github.io/langgraph/prebuilt/?ref=blog.langchain.dev"&gt;prebuilt agents&lt;/a&gt; built on top of LangGraph, in both Python and JavaScript.&lt;/p&gt;&lt;p&gt;Over the past three weeks, we&amp;#x2019;ve already released a few of these:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/hinthornw/trustcall?ref=blog.langchain.dev"&gt;Trustcall&lt;/a&gt;: for doing reliable structured extraction&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/langchain-ai/langgraph-supervisor-py?ref=blog.langchain.dev"&gt;LangGraph Supervisor&lt;/a&gt;: for getting started with a supervisor multi-agent architecture&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/langchain-ai/langmem?ref=blog.langchain.dev"&gt;LangMem&lt;/a&gt;: for long term memory&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/langchain-ai/langgraph-swarm-py?ref=blog.langchain.dev"&gt;LangGraph Swarm&lt;/a&gt;: for getting started with a swarm multi-agent architecture&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We believe that these prebuilt libraries can combine the best of both worlds:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;They make it easy to get started with common agent patterns&lt;/li&gt;&lt;li&gt;They are built on top of LangGraph, so if you want to modify them it&amp;#x2019;s easy and familiar&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We hope that this will foster a large collection of prebuilt agents built by the community. To that end, we have added &lt;a href="https://langchain-ai.github.io/langgraph/prebuilt/?ref=blog.langchain.dev#contributing-your-library"&gt;instructions&lt;/a&gt; for creating your own prebuilt package and adding it to our registry of agents. We have seen this work well with LangChain integrations. We have over 700 integrations, a large amount maintained by the community in third party packages. We hope the same will happen with LangGraph prebuilt agents.&lt;/p&gt; </content:encoded><pubDate>Thu, 27 Feb 2025 15:09:15 +0000</pubDate></item><item><title>How healthcare organizations are using generative AI search and agents</title><link>https://blog.google/products/google-cloud/himss-2025/</link><description>Doctor discussing health data with an older patient at a table. Charts and a checklist are visible on screens.</description><content:encoded>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_in_Healthcare.max-600x600.format-webp.webp"&gt;Google Cloud and healthcare organizations share new partnerships at HIMSS 2025. </content:encoded><pubDate>Mon, 03 Mar 2025 14:00:00 +0000</pubDate></item><item><title>3 new ways we’re working to protect and restore nature using AI</title><link>https://blog.google/outreach-initiatives/entrepreneurs/ai-nature-climate-accelerator-nonprofits-speciesnet/</link><description>Three vertical panels: a jaguar with a blue bounding box, a map of South America with green and red areas, and a man in a forest with recording equipment.</description><content:encoded>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NatureAI_Hero-5.max-600x600.format-webp.webp"&gt;Learn more about Google for Startups Accelerator: AI for Nature and Climate, as well as other new efforts to use technology to preserve our environment. </content:encoded><pubDate>Mon, 03 Mar 2025 16:00:00 +0000</pubDate></item><item><title>The latest AI news we announced in February</title><link>https://blog.google/technology/ai/google-ai-updates-february-2025/</link><description>an MP4 carousel of images and illustrations, included Sundar Pichai speaking at a lectern and an illustration of two people coding</description><content:encoded>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Feb-AI-Recap-_-SS.max-600x600.format-webp.webp"&gt;Here are Google’s latest AI updates from February 2025 </content:encoded><pubDate>Mon, 03 Mar 2025 19:45:00 +0000</pubDate></item><item><title>The latest AI news we announced in February</title><link>https://blog.google/technology/ai/google-ai-updates-february-2025/</link><description>an MP4 carousel of images and illustrations, included Sundar Pichai speaking at a lectern and an illustration of two people coding</description><content:encoded>&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Feb-AI-Recap-_-SS.max-600x600.format-webp.webp"&gt;Here are Google’s latest AI updates from February 2025 </content:encoded><pubDate>Mon, 03 Mar 2025 19:45:00 +0000</pubDate></item></channel></rss>