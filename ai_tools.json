{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "AI Daily Digest - Ai Tools",
  "home_page_url": "https://your-github-pages-url/",
  "feed_url": "https://your-github-pages-url/ai_tools.json",
  "description": "Latest AI news and updates from ai tools sources",
  "items": [
    {
      "id": "https://aws.amazon.com/blogs/machine-learning/customize-deepseek-r1-distilled-models-using-amazon-sagemaker-hyperpod-recipes-part-1/",
      "url": "https://aws.amazon.com/blogs/machine-learning/customize-deepseek-r1-distilled-models-using-amazon-sagemaker-hyperpod-recipes-part-1/",
      "title": "Customize DeepSeek-R1 distilled models using Amazon SageMaker HyperPod recipes – Part 1",
      "content_html": "<p>Increasingly, organizations across industries are turning to <a href=\"https://aws.amazon.com/generative-ai/\" rel=\"noopener\" target=\"_blank\">generative AI</a> <a href=\"https://aws.amazon.com/what-is/foundation-models/\" rel=\"noopener\" target=\"_blank\">foundation models</a> (FMs) to enhance their applications. To achieve optimal performance for specific use cases, customers are adopting and adapting these FMs to their unique domain requirements. This need for customization has become even more pronounced with the emergence of new models, such as those released by <a href=\"https://huggingface.co/deepseek-ai\" rel=\"noopener\" target=\"_blank\">DeepSeek</a>.</p> \n<p>However, customizing DeepSeek models effectively while managing computational resources remains a significant challenge. Tuning model architecture requires technical expertise, training and fine-tuning parameters, and managing distributed training infrastructure, among others. This often forces companies to choose between model performance and practical implementation constraints, creating a critical need for more accessible and streamlined model customization solutions.</p> \n<p>In this two-part series, we discuss how you can reduce the DeepSeek model customization complexity by using the pre-built fine-tuning workflows (also called “recipes”) for both DeepSeek-R1 model and its distilled variations, released as part of <a href=\"https://aws.amazon.com/blogs/aws/accelerate-foundation-model-training-and-fine-tuning-with-new-amazon-sagemaker-hyperpod-recipes/\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker HyperPod recipes</a>.</p> \n<p>In this first post, we will build a solution architecture for fine-tuning DeepSeek-R1 distilled models and demonstrate the approach by providing a step-by-step example on customizing the <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" rel=\"noopener\" target=\"_blank\">DeepSeek-R1 Distill Qwen 7b</a> model using recipes, achieving an average of 25% on all the Rouge scores, with a maximum of 49% on Rouge 2 score with both <a href=\"https://aws.amazon.com/sagemaker/hyperpod/\" rel=\"noopener\" target=\"_blank\">SageMaker HyperPod </a>and <a href=\"https://aws.amazon.com/sagemaker/train/\" rel=\"noopener\" target=\"_blank\">SageMaker training jobs</a>. The second part of the series will focus on fine-tuning the <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1\" rel=\"noopener\" target=\"_blank\">DeepSeek-R1 671b</a> model itself.</p> \n<p>At the time of this writing, the DeepSeek-R1 model and its distilled variations for Llama and Qwen were the latest released recipe. Check out <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\" rel=\"noopener\" target=\"_blank\">sagemaker-hyperpod-recipes on GitHub</a> for the latest released recipes, including support for fine-tuning the DeepSeek-R1 671b parameter model.</p> \n<h2>Amazon SageMaker HyperPod recipes</h2> \n<p>At re:Invent 2024, we announced the general availability of <a href=\"https://aws.amazon.com/blogs/aws/accelerate-foundation-model-training-and-fine-tuning-with-new-amazon-sagemaker-hyperpod-recipes/\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker HyperPod recipes</a>. SageMaker HyperPod recipes help data scientists and developers of all skill sets to get started training and fine-tuning popular publicly available generative AI models in minutes with state-of-the-art training performance. These recipes include a training stack validated by <a href=\"https://aws.amazon.com/\" rel=\"noopener\" target=\"_blank\">Amazon Web Services (AWS)</a>, which removes the tedious work of experimenting with different model configurations, minimizing the time it takes for iterative evaluation and testing. They automate several critical steps, such as loading training datasets, applying distributed training techniques, automating checkpoints for faster recovery from faults, and managing the end-to-end training loop.</p> \n<p>Recipes, paired with the resilient infrastructure of AWS, (<a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker HyperPod</a> and <a href=\"https://aws.amazon.com/sagemaker-ai/train/\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker Model Training</a>) provide a resilient training environment for fine-tuning FMs such as DeepSeek-R1 with out-of-the-box customization.</p> \n<p>To help customers quickly use DeepSeek’s powerful and cost-efficient models to accelerate generative AI innovation, we released new recipes to fine-tune six DeepSeek models, including DeepSeek-R1 distilled Llama and Qwen models using <a href=\"https://arxiv.org/abs/2412.13337\" rel=\"noopener\" target=\"_blank\">supervised fine-tuning</a> (SFT), <a href=\"https://arxiv.org/abs/2305.14314\" rel=\"noopener\" target=\"_blank\">Quantized Low-Rank Adaptation</a> (QLoRA), <a href=\"https://arxiv.org/abs/2106.09685\" rel=\"noopener\" target=\"_blank\">Low-Rank Adaptation</a> (LoRA) techniques. In this post, we introduce these new recipes and walk you through a solution to fine-tune a DeepSeek Qwen 7b model for an advanced medical reasoning use case.</p> \n<h2>Solution overview</h2> \n<p>At its core, as depicted in the following diagram, the recipe architecture implements a hierarchical workflow that begins with a recipe specification that covers a comprehensive configuration defining the training parameters, model architecture, and distributed training strategies. These recipes are processed through the HyperPod recipe launcher, which serves as the orchestration layer responsible for launching a job on the corresponding architecture. The launcher interfaces with underlying cluster management systems such as SageMaker HyperPod (Slurm or Kubernetes) or training jobs, which handle resource allocation and scheduling. It’s a familiar NeMo-style launcher with which you can choose a recipe and run it on your infrastructure of choice (SageMaker HyperPod or training).</p> \n<p>For example, after <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes?tab=readme-ov-file#fine-tuning\" rel=\"noopener\" target=\"_blank\">choosing your recipe</a>, you can pre-train or fine-tune a model by running <code>python3 main.py recipes=recipe-name</code>. Alternatively, you can use a launcher script, which is a bash script that is preconfigured to run the chosen training or fine-tuning job on your cluster. You can check out <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/main.py\" rel=\"noopener\" target=\"_blank\">main.py</a> (NeMo style launcher) and <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/tree/main/launcher_scripts/deepseek\" rel=\"noopener\" target=\"_blank\">launcher scripts for DeepSeek</a> on the <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/tree/main\" rel=\"noopener\" target=\"_blank\">GitHub repository hosting SageMaker HyperPod recipes</a>.</p> \n<p>A key component of this architecture is the HyperPod training adapter for NeMo, which is built on the <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html\" rel=\"noopener\" target=\"_blank\">NVIDIA NeMo framework</a> and <a href=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/index.html\" rel=\"noopener\" target=\"_blank\">Neuronx Distributed training package</a>, which loads data, creates models, and facilitates efficient data parallelism, model parallelism, and hybrid parallelism strategies, which enables optimal utilization of computational resources across the distributed infrastructure. The architecture’s modular design allows for scalability and flexibility, making it particularly effective for training LLMs that require distributed computing capabilities.</p> \n<p><img alt=\"\" class=\"aligncenter size-full wp-image-100522\" height=\"732\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/26/Picture1-8.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"844\" /></p> \n<p>You can run these recipes using SageMaker HyperPod or as SageMaker training jobs. For organizations that require granular control over training infrastructure and extensive customization options, SageMaker HyperPod is the ideal choice. SageMaker training jobs, on the other hand, is tailored for organizations that want a fully managed experience for their training workflows. To learn more details about these service features, refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/generative-ai-foundation-model-training-on-amazon-sagemaker/\" rel=\"noopener\" target=\"_blank\">Generative AI foundation model training on Amazon SageMaker</a>.</p> \n<p>In the next sections, we go over the solution architecture for these services before presenting a step-by-step implementation example for each.</p> \n<h3>SageMaker HyperPod</h3> \n<p>To submit jobs using SageMaker HyperPod, you can use the HyperPod recipes launcher, which provides an straightforward mechanism to run recipes on both Slurm and Kubernetes. After you choose your orchestrator, you can choose your recipe’s launcher and have it run on your HyperPod cluster. The launcher will interface with your cluster with Slurm or Kubernetes native constructs. For this post, we use the HyperPod recipes launcher mechanism to run the training on a Slurm cluster. The following image shows the solution architecture for SageMaker HyperPod.</p> \n<p><img alt=\"\" class=\"aligncenter size-full wp-image-100531\" height=\"756\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/26/hp-sceptor-ezgif.com-optimize-1.gif\" style=\"margin: 10px 0px 10px 0px;\" width=\"1344\" /></p> \n<h3>SageMaker training jobs</h3> \n<p>The workflow for SageMaker training jobs begins with an API request that interfaces with the SageMaker control plane, which manages the orchestration of training resources. The system uses the training jobs launcher to efficiently run workloads on a managed cluster.</p> \n<p>The architecture uses <a href=\"https://aws.amazon.com/ecr/\" rel=\"noopener\" target=\"_blank\">Amazon Elastic Container Registry</a> (Amazon ECR) for container image management. Training jobs are executed across a distributed cluster, with seamless integration to multiple storage solutions, including <a href=\"https://aws.amazon.com/s3/\" rel=\"noopener\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3), <a href=\"https://aws.amazon.com/efs/\" rel=\"noopener\" target=\"_blank\">Amazon Elastic File Storage</a> (Amazon EFS), and <a href=\"https://aws.amazon.com/fsx/lustre/\" rel=\"noopener\" target=\"_blank\">Amazon FSx for Lustre</a>. All of this runs under the SageMaker managed environment, providing optimal resource utilization and security.</p> \n<p>This design simplifies the complexity of distributed training while maintaining the flexibility needed for diverse machine learning (ML) workloads, making it an ideal solution for enterprise AI development. The following image shows the solution architecture for SageMaker training jobs.</p> \n<p><img alt=\"\" class=\"aligncenter size-full wp-image-100533\" height=\"756\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/26/tj-sceptor-ezgif.com-optimize.gif\" style=\"margin: 10px 0px 10px 0px;\" width=\"1344\" /></p> \n<h2>Solution walkthrough</h2> \n<p>For this solution, consider a use case for a healthcare industry startup that aims to create an accurate, medically verified chat assistant application that bridges complex medical information with patient-friendly explanations. By fine-tuning DeepSeek-R1 Distill Qwen 7b using the <a href=\"https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT\" rel=\"noopener\" target=\"_blank\">FreedomIntelligence/medical-o1-reasoning-SFT</a> dataset, you can use its medical reasoning capabilities to produce content that maintains clinical accuracy.</p> \n<h2>Prerequisites</h2> \n<p>You need to complete the following prerequisites before you can run the DeepSeek-R1 Distill Qwen 7B model fine-tuning notebook.</p> \n<ol> \n <li>Make the following quota increase requests for SageMaker. You need to request a minimum of one <code>p4d.24xlarge</code> instance (with 8 x NVIDIA A100 GPUs) ranging to a maximum of two <code>p4d.24xlarge</code> instances (depending on time-to-train and cost-to-train trade-offs for your use case).</li> \n</ol> \n<p>On the <a href=\"https://docs.aws.amazon.com/servicequotas/latest/userguide/intro.html\" rel=\"noopener\" target=\"_blank\">Service Quotas</a> console, request the following SageMaker quotas:</p> \n<ul> \n <li> \n  <ul> \n   <li>P4 instances (<code>p4d.24xlarge</code>) for training job usage: 1–2</li> \n   <li>P4 instances (<code>p4d.24xlarge</code>) for HyperPod clusters (“<code>ml.p4d.24xlarge</code> for cluster usage“): 1-2</li> \n  </ul> </li> \n</ul> \n<ol start=\"2\"> \n <li>If you choose to use HyperPod clusters to run your training, set up a HyperPod Slurm cluster following the documentation at <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/smcluster-getting-started.html\" rel=\"noopener\" target=\"_blank\">Tutuorial for getting started with SageMaker HyperPod</a>. Alternatively, you can use the <a href=\"https://aws.amazon.com/cloudformation/\" rel=\"noopener\" target=\"_blank\">AWS CloudFormation</a> template provided in the AWS Workshop Studio at <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod/en-US/00-setup/02-own-account\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker HyperPod Own Account</a> and follow the instructions to <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod/en-US/01-cluster\" rel=\"noopener\" target=\"_blank\">set up a cluster</a> and a development environment to access and submit jobs to the cluster.</li> \n <li>(Optional) If you choose to use <a href=\"https://aws.amazon.com/sagemaker/studio/\" rel=\"noopener\" target=\"_blank\">SageMaker training jobs</a>, you can create an Amazon SageMaker Studio domain (refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html\" rel=\"noopener\" target=\"_blank\">Use quick setup for Amazon SageMaker AI</a>) to access <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-jl-user-guide.html\" rel=\"noopener\" target=\"_blank\">Jupyter notebooks</a> with the preceding role. (You can use JupyterLab in your local setup, too.)</li> \n</ol> \n<ul> \n <li> \n  <ul> \n   <li>Create an <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#:~:text=the%20following%20procedures.-[%E2%80%A6]xecution%20role,-Use%20the%20following%20(\" rel=\"noopener\" target=\"_blank\">role</a> with managed policies <code>AmazonSageMakerFullAccess</code> and <code>AmazonS3FullAccess</code> to give required access to SageMaker to run the examples.</li> \n  </ul> </li> \n</ul> \n<ol start=\"2\"> \n <li>Clone the GitHub repository with the assets for this deployment. This repository consists of a notebook that references training assets:</li> \n</ol> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">git clone https://github.com/aws-samples/sagemaker-distributed-training-workshop.git \ncd 18_sagemaker_training_recipes/ft_deepseek_qwen_lora</code></pre> \n</div> \n<p>Next, we run the <a href=\"https://github.com/aws-samples/sagemaker-distributed-training-workshop/blob/main/18_sagemaker_training_recipes/ft_deepseek_qwen_lora/model_trainer_deepseek_r1_recipe_lora.ipynb\" rel=\"noopener\" target=\"_blank\">model_trainer_deepseek_r1_recipe_lora.ipynb</a> notebook to fine-tune the DeepSeek-R1 model using QLoRA on SageMaker.</p> \n<h2>Prepare the dataset</h2> \n<p>To prepare the dataset, you need to load the <code>FreedomIntelligence/medical-o1-reasoning-SFT dataset</code>, tokenize and chunk the dataset, and configure the data channels for SageMaker training on Amazon S3. Complete the following steps:</p> \n<ol> \n <li>Format the dataset by applying the prompt format for DeepSeek-R1 Distill Qwen 7B:</li> \n</ol> \n<pre><code class=\"lang-python\">def generate_prompt(data_point):\n&nbsp;&nbsp; &nbsp;full_prompt = f\"\"\"\n&nbsp;&nbsp; &nbsp;Below is an instruction that describes a task, paired with an input that provides further context.\n&nbsp;&nbsp; &nbsp;Write a response that appropriately completes the request.\n&nbsp;&nbsp; &nbsp;Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n&nbsp;&nbsp; &nbsp;### Instruction:\n&nbsp;&nbsp; &nbsp;You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n&nbsp;&nbsp; &nbsp;Please answer the following medical question.\n\n&nbsp;&nbsp; &nbsp;### Question:\n&nbsp;&nbsp; &nbsp;{data_point[\"Question\"]}\n\n&nbsp;&nbsp; &nbsp;### Response:\n&nbsp;&nbsp; &nbsp;{data_point[\"Complex_CoT\"]}\n\n&nbsp;&nbsp; &nbsp;\"\"\"\n&nbsp;&nbsp; &nbsp;return {\"prompt\": full_prompt.strip()}</code></pre> \n<ol start=\"2\"> \n <li>Load the <a href=\"https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT\" rel=\"noopener\" target=\"_blank\">FreedomIntelligence/medical-o1-reasoning-SFT</a> dataset and split it into training and validation datasets:</li> \n</ol> \n<pre><code class=\"lang-python\"># Load dataset from the hub\ntrain_set = load_dataset(dataset_name, 'en', split=\"train[5%:]\")\ntest_set = load_dataset(dataset_name, 'en', split=\"train[:5%]\")\n\n...\n\ntrain_dataset = train_set.map(\n&nbsp;&nbsp; &nbsp;generate_and_tokenize_prompt,\n&nbsp;&nbsp; &nbsp;remove_columns=columns_to_remove,\n&nbsp;&nbsp; &nbsp;batched=False\n)\n\ntest_dataset = test_set.map(\n&nbsp;&nbsp; &nbsp;generate_and_tokenize_prompt,\n&nbsp;&nbsp; &nbsp;remove_columns=columns_to_remove,\n&nbsp;&nbsp; &nbsp;batched=False\n)</code></pre> \n<ol start=\"3\"> \n <li>Load the DeepSeek-R1 Distill Qwen 7B tokenizer from the Hugging Face Transformers library and generate tokens for the train and validation datasets:</li> \n</ol> \n<pre><code class=\"lang-python\">model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\nmax_seq_length=1024\n\n# Initialize a tokenizer by loading a pre-trained tokenizer configuration, using the fast tokenizer implementation if available.\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n\n...\n\ntrain_dataset = train_dataset.map(tokenize, remove_columns=[\"prompt\"])\ntest_dataset = test_dataset.map(tokenize, remove_columns=[\"prompt\"])</code></pre> \n<ol start=\"4\"> \n <li>Prepare the training and validation datasets for SageMaker training by saving them as <code>arrow</code> files, which is required by SageMaker HyperPod recipes, and constructing the S3 paths where these files will be uploaded:</li> \n</ol> \n<pre><code class=\"lang-python\">train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train\"\nval_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test\"\n\ntrain_dataset.save_to_disk(train_dataset_s3_path)\nval_dataset.save_to_disk(val_dataset_s3_path)</code></pre> \n<p>The dataset above will be used in the examples for both SageMaker training jobs and SageMaker HyerPod.</p> \n<h2>Option A: Fine-tune using SageMaker training jobs</h2> \n<p>To fine-tune the model using SageMaker training jobs with recipes, this example uses the ModelTrainer class.</p> \n<p>The ModelTrainer class is a newer and more intuitive approach to model training that significantly enhances user experience and supports distributed training, Build Your Own Container (BYOC), and recipes. For additional information about ModelTrainer, you can refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/accelerate-your-ml-lifecycle-using-the-new-and-improved-amazon-sagemaker-python-sdk-part-1-modeltrainer/?t\" rel=\"noopener\" target=\"_blank\">Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK – Part 1: ModelTrainer</a></p> \n<p>To set up the fine-tuning workload, complete the following steps:</p> \n<ol> \n <li>Select the instance type, the container image for the training job, and define the checkpoint path where the model will be stored:</li> \n</ol> \n<pre><code class=\"lang-python\">instance_type = \"ml.p4d.24xlarge\"\n\nimage_uri = (\n&nbsp;&nbsp; &nbsp;f\"658645717510.dkr.ecr.{sagemaker_session.boto_session.region_name}.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121\"\n)\n\ncheckpoint_s3_path = f\"s3://{bucket_name}/deepseek-r1-distilled-qwen-7b-recipe-lora/checkpoints\"</code></pre> \n<ol start=\"2\"> \n <li>Create the ModelTrainer function to encapsulate the training setup from a selected recipe:</li> \n</ol> \n<pre><code class=\"lang-python\">from sagemaker.modules.configs import CheckpointConfig, Compute, InputData, SourceCode, StoppingCondition\nfrom sagemaker.modules.distributed import Torchrun\nfrom sagemaker.modules.train import ModelTrainer\n\ninstance_count = 1\n\n# Working override for custom dataset\nrecipe_overrides = {\n&nbsp; &nbsp; ...\n&nbsp;&nbsp; &nbsp;\"trainer\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"num_nodes\": instance_count,\n&nbsp; &nbsp; &nbsp; &nbsp; ...\n&nbsp;&nbsp; &nbsp;},\n&nbsp; &nbsp; ...\n&nbsp;&nbsp; &nbsp;\"use_smp_model\": False,&nbsp;# Required for PEFT\n&nbsp;&nbsp; &nbsp;\"model\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"hf_model_name_or_path\": model_id,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"data\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"train_dir\": \"/opt/ml/input/data/train\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"val_dir\": \"/opt/ml/input/data/test\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;},\n}\n\n# Define the compute\ncompute_configs = Compute(\n&nbsp;&nbsp; &nbsp;instance_type=instance_type,\n&nbsp;&nbsp; &nbsp;instance_count=instance_count,\n&nbsp;&nbsp; &nbsp;keep_alive_period_in_seconds=0\n)\n\nmodel_trainer = ModelTrainer.from_recipe(\n&nbsp;&nbsp; &nbsp;training_image=image_uri,\n&nbsp;&nbsp; &nbsp;training_recipe=\"fine-tuning/deepseek/hf_deepseek_r1_distilled_qwen_7b_seq8k_gpu_lora\",\n&nbsp;&nbsp; &nbsp;recipe_overrides=recipe_overrides,\n&nbsp;&nbsp; &nbsp;requirements=\"./requirements.txt\",\n&nbsp;&nbsp; &nbsp;compute=compute_configs,\n&nbsp; &nbsp; ...\n&nbsp;&nbsp; &nbsp;checkpoint_config=CheckpointConfig(\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3_uri=f\"{checkpoint_s3_path}/{job_prefix}\"\n&nbsp;&nbsp; &nbsp;),\n)</code></pre> \n<p>You can point to the specific recipe with the <code>training_recipe</code> argument and override the recipe arguments by providing a dictionary as argument of <code>recipe_overrides</code>. In the previous example:</p> \n<ul> \n <li><code>num_nodes</code>: Indicates the number of instances that will be used for the fine-tuning execution</li> \n <li><code>checkpoint_dir</code>: Location in the container where the job will save model checkpoints</li> \n</ul> \n<p>The ModelTrainer class simplifies the experience by encapsulating code and training setup directly from the selected recipe. In this example:</p> \n<ul> \n <li><code>training_recipe</code>: <code>hf_deepseek_r1_distilled_qwen_7b_seq8k_gpu_lora</code> is defining fine-tuning setup for the LoRA technique</li> \n</ul> \n<ol start=\"3\"> \n <li>Set up the input channels for ModelTrainer by creating an <a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/model_trainer.html#sagemaker.modules.configs.InputData\" rel=\"noopener\" target=\"_blank\">InputData</a> objects from the provided S3 bucket paths for the training and test and validation datasets</li> \n <li>Submit the training job:</li> \n</ol> \n<pre><code class=\"lang-python\"># starting the train job with our uploaded datasets as input\nmodel_trainer.train(input_data_config=data, wait=True)</code></pre> \n<h2>Option B: Fine-tune using SageMaker HyperPod with Slurm</h2> \n<p>To fine-tune the model using HyperPod, make sure your cluster is up and ready by following the prerequisites. To access the login or head node of the HyperPod Slurm cluster from your development environment, follow the login instructions at <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod/en-US/01-cluster/05-ssh\" rel=\"noopener\" target=\"_blank\">Log in to your cluster</a> in the Amazon SageMaker HyperPod workshop.</p> \n<p>Alternatively, you can also use <a href=\"https://aws.amazon.com/systems-manager/\" rel=\"noopener\" target=\"_blank\">AWS Systems Manager</a> and run a command like the following to start the session. You can find the cluster ID, instance group name, and instance ID on the <a href=\"https://console.aws.amazon.com/sagemaker/\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker console</a>.</p> \n<pre><code class=\"lang-bash\">aws ssm start-session --target sagemaker-cluster:[cluster-id]_[instance-group-name]-[instance-id] --region region_name</code></pre> \n<ol> \n <li>In the cluster’s login or head node, run the following commands to set up the environment. Run <code>sudo su - ubuntu</code> to run the remaining commands as the root user unless you have a specific user ID to access the cluster and your POSIX user is created through a lifecycle script on the cluster. Refer to the <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod/en-US/05-advanced/01-multi-user\" rel=\"noopener\" target=\"_blank\">multi-user</a> setup for more details.</li> \n</ol> \n<pre><code class=\"lang-bash\"># create a virtual environment&nbsp;\npython3 -m venv ${PWD}/venv\nsource venv/bin/activate\n\n# clone the recipes repository and set up the environment\ngit clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git\ncd sagemaker-hyperpod-recipes\npip3 install -r requirements.txt</code></pre> \n<ol start=\"2\"> \n <li>Create a squash file using <a href=\"https://github.com/NVIDIA/enroot\" rel=\"noopener\" target=\"_blank\">Enroot</a> to run the job on the cluster. Enroot runtime offers GPU acceleration, rootless container support, and seamless integration with high performance computing (HPC) environments, making it ideal for running our workflows securely.</li> \n</ol> \n<pre><code class=\"lang-bash\"># create a squash file using Enroot\nREGION=&lt;region&gt;\nIMAGE=\"658645717510.dkr.ecr.${REGION}.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121\"\naws ecr get-login-password --region \"${REGION}\" | docker login --username AWS --password-stdin 658645717510.dkr.ecr.${REGION}.amazonaws.com\nenroot import -o $PWD/smdistributed-modelparallel.sqsh dockerd://${IMAGE}</code></pre> \n<ol start=\"3\"> \n <li>After you’ve created the squash file, update the <code>recipes_collection/config.yaml</code> file with the absolute path to the squash file (created in the preceding step), and update the <code>instance_type</code> if needed. The final config file should have the following parameters:</li> \n</ol> \n<pre><code class=\"lang-yaml\">...\n\ncluster_type:&nbsp;slurm&nbsp;\n...\n\ninstance_type:&nbsp;p4d.24xlarge\n...\n\ncontainer:&nbsp;/fsx/&lt;path-to-smdistributed-modelparallel&gt;.sqsh\n...</code></pre> \n<ol start=\"4\"> \n <li>Download the prepared dataset that you uploaded to S3 into the <a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html\" rel=\"noopener\" target=\"_blank\">FSx for Lustre</a> volume attached to the cluster. Run the following commands to download the files from Amazon S3:</li> \n</ol> \n<pre><code class=\"lang-bash\">aws s3 cp&nbsp;s3://{bucket_name}/{input_path}/train /fsx/ubuntu/deepseek/data/train --recursive\naws s3 cp s3://{bucket_name}/{input_path}/test /fsx/ubuntu/deepseek/data/test --recursive</code></pre> \n<ol start=\"5\"> \n <li>Update the launcher script for fine-tuning the DeepSeek-R1 Distill Qwen 7B model. The launcher scripts serve as convenient wrappers for executing the training script <code>main.py</code> file), which streamlines the process of fine-tuning and parameter adjustment. For fine-tuning the DeepSeek-R1 Qwen 7B model, you can find the specific script at:</li> \n</ol> \n<pre><code class=\"lang-bash\">launcher_scripts/deepseek/run_hf_deepseek_r1_qwen_7b_seq16k_gpu_fine_tuning.sh</code></pre> \n<ol start=\"6\"> \n <li>Before running the script, you need to modify the location of the training and validation files and update the HuggingFace model ID and optionally the access token for private models and datasets. The script should look like the following (update <code>recipes.trainer.num_nodes</code> if you’re using a multi-node cluster):</li> \n</ol> \n<pre><code class=\"lang-bash\">SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-\"$(pwd)\"}\n\nHF_MODEL_NAME_OR_PATH=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # HuggingFace pretrained model name or path\nHF_ACCESS_TOKEN=\"hf_xxxx\" # Optional HuggingFace access token\n\nTRAIN_DIR=\"/fsx/ubuntu/deepseek/data/train\" # Location of training dataset&nbsp;\nVAL_DIR=\"/fsx/ubuntu/deepseek/data/test\" # Location of validation dataset\n\nEXP_DIR=\"/fsx/ubuntu/deepseek/results\" # Location to save experiment info including logging, checkpoints, etc\n\nHYDRA_FULL_ERROR=1 python3 \"${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py\" \\\n&nbsp;&nbsp; &nbsp;recipes=fine-tuning/deepseek/hf_deepseek_r1_distilled_qwen_7b_seq16k_gpu_fine_tuning \\\n&nbsp;&nbsp; &nbsp;base_results_dir=\"${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results\" \\\n&nbsp;&nbsp; &nbsp;recipes.run.name=\"hf-deepseek-r1-distilled-qwen-7b-fine-tuning\" \\\n&nbsp;&nbsp; &nbsp;recipes.exp_manager.exp_dir=\"$EXP_DIR\" \\\n&nbsp;&nbsp; &nbsp;recipes.trainer.num_nodes=1 \\\n&nbsp;&nbsp; &nbsp;recipes.model.data.train_dir=\"$TRAIN_DIR\" \\\n&nbsp;&nbsp; &nbsp;recipes.model.data.val_dir=\"$VAL_DIR\" \\\n&nbsp;&nbsp; &nbsp;recipes.model.hf_model_name_or_path=\"$HF_MODEL_NAME_OR_PATH\" \\\n&nbsp;&nbsp; &nbsp;recipes.model.hf_access_token=\"$HF_ACCESS_TOKEN\" \\</code></pre> \n<p>You can view the recipe for this fine-tuning task under, overriding any additional parameters as needed:</p> \n<pre><code class=\"lang-bash\">recipes_collection/recipes/fine-tuning/deepseek/hf_deepseek_r1_distilled_qwen_7b_seq16k_gpu_fine_tuning.yaml</code></pre> \n<ol start=\"7\"> \n <li>Submit the job by running the launcher script:</li> \n</ol> \n<pre><code class=\"lang-bash\">bash launcher_scripts/deepseek/run_hf_deepseek_r1_qwen_7b_seq16k_gpu_fine_tuning.sh</code></pre> \n<p>You can monitor the job using Slurm commands such as <code>squeue</code> and <code>scontrol</code> show to view the status of the job and the corresponding logs. After the job is complete, the trained model will also be available in the results folder, as shown in the following code:</p> \n<pre><code class=\"lang-bash\">cd results\n&nbsp;ls -R\n.:\ncheckpoints &nbsp;experiment\n\n./checkpoints:\nfull\n\n./checkpoints/full:\nsteps_50\n\n./checkpoints/full/steps_50:\nconfig.json &nbsp;pytorch_model.bin\n\n./experiment:\n...</code></pre> \n<ol start=\"8\"> \n <li>Upload the fine-tuned model checkpoint to Amazon S3 for evaluating the model using the validation data:</li> \n</ol> \n<pre><code class=\"lang-bash\">aws s3 cp /fsx/&lt;path_to_checkpoint&gt;&nbsp;s3://{bucket_name}/{model_prefix}/qwen7b --recursive</code></pre> \n<h2>Evaluate the fine-tuned model</h2> \n<p>To objectively evaluate your fine-tuned model, you can run an evaluation job on the validation portion of the dataset.</p> \n<p>You can run a SageMaker training job and use ROUGE metrics (ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-L-Sum), which measure the similarity between machine-generated text and human-written reference text. The SageMaker training job will compute ROUGE metrics for both the base DeepSeek-R1 Distill Qwen 7B model and the fine-tuned one. You can access the code sample for ROUGE evaluation in the <a href=\"https://github.com/aws-samples/sagemaker-distributed-training-workshop/blob/main/18_sagemaker_training_recipes/ft_deepseek_qwen_lora/scripts/evaluate_recipe.py\" rel=\"noopener\" target=\"_blank\">sagemaker-distributed-training-workshop</a> on GitHub. Please refer this <a href=\"https://github.com/aws-samples/sagemaker-distributed-training-workshop/tree/main/18_sagemaker_training_recipes/ft_deepseek_qwen_lora\" rel=\"noopener\" target=\"_blank\">notebook</a> for details.</p> \n<p>Complete the following steps:</p> \n<ol> \n <li>Define the S3 path where the fine-tuned checkpoints are stored, the instance_type, and the image uri to use in the training job:</li> \n</ol> \n<pre><code class=\"lang-python\">trained_model&nbsp;= &lt;S3_PATH&gt;\ninstance_type = \"ml.p4d.24xlarge\"\n\nimage_uri = sagemaker.image_uris.retrieve(\n&nbsp;&nbsp; &nbsp;framework=\"pytorch\",\n&nbsp;&nbsp; &nbsp;region=sagemaker_session.boto_session.region_name,\n&nbsp;&nbsp; &nbsp;version=\"2.4\",\n&nbsp;&nbsp; &nbsp;instance_type=instance_type,\n&nbsp;&nbsp; &nbsp;image_scope=\"training\"\n)\n#763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.4-gpu-py311</code></pre> \n<ol start=\"2\"> \n <li>Create the ModelTrainer function to encapsulate the evaluation script and define the input data:</li> \n</ol> \n<pre><code class=\"lang-python\">from sagemaker.modules.configs import Compute, InputData, OutputDataConfig, SourceCode, StoppingCondition\nfrom sagemaker.modules.distributed import Torchrun\nfrom sagemaker.modules.train import ModelTrainer\n\n# Define the script to be run\nsource_code = SourceCode(\n&nbsp;&nbsp; &nbsp;source_dir=\"./scripts\",\n&nbsp;&nbsp; &nbsp;requirements=\"requirements.txt\",\n&nbsp;&nbsp; &nbsp;entry_script=\"evaluate_recipe.py\",\n)\n\n# Define the compute\n...\n\n# Define the ModelTrainer\nmodel_trainer = ModelTrainer(\n&nbsp;&nbsp; &nbsp;training_image=image_uri,\n&nbsp;&nbsp; &nbsp;source_code=source_code,\n&nbsp;&nbsp; &nbsp;compute=compute_configs,\n&nbsp; &nbsp; ...\n&nbsp;&nbsp; &nbsp;hyperparameters={\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"model_id\": model_id, &nbsp;# Hugging Face model id\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"dataset_name\": dataset_name\n&nbsp;&nbsp; &nbsp;}\n)\n\n# Pass the input data\ntrain_input = InputData(\n   channel_name=\"adapterdir\",\n   data_source=trained_model,\n)\n\ntest_input = InputData(\n   channel_name=\"testdata\",\n   data_source=test_dataset_s3_path, # S3 path where training data is stored\n)\n\n# Check input channels configured\ndata = [train_input, test_input]</code></pre> \n<ol start=\"3\"> \n <li>Submit the training job:</li> \n</ol> \n<pre><code class=\"lang-python\"># starting the train job with our uploaded datasets as input\nmodel_trainer.train(input_data_config=data, wait=True)</code></pre> \n<p>The following table shows the task output for the fine-tuned model and the base model.</p> \n<table border=\"2px\" cellpadding=\"5px\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td width=\"88\"><span style=\"color: #ffffff;\"><strong>Model</strong></span></td> \n   <td width=\"88\"><span style=\"color: #ffffff;\"><strong>Rouge 1</strong></span></td> \n   <td width=\"88\"><span style=\"color: #ffffff;\"><strong>Rouge 2</strong></span></td> \n   <td width=\"88\"><span style=\"color: #ffffff;\"><strong>Rouge L</strong></span></td> \n   <td width=\"88\"><span style=\"color: #ffffff;\"><strong>Rouge L Sum</strong></span></td> \n  </tr> \n  <tr> \n   <td width=\"88\">Base</td> \n   <td width=\"88\">0.36362</td> \n   <td width=\"88\">0.08739</td> \n   <td width=\"88\">0.16345</td> \n   <td width=\"88\">0.3204</td> \n  </tr> \n  <tr> \n   <td width=\"88\">Fine-tuned</td> \n   <td width=\"88\">0.44232</td> \n   <td width=\"88\">0.13022</td> \n   <td width=\"88\">0.17769</td> \n   <td width=\"88\">0.38989</td> \n  </tr> \n  <tr> \n   <td width=\"88\">% Difference</td> \n   <td width=\"88\"><span style=\"color: #008000;\">21.64207</span></td> \n   <td width=\"88\"><span style=\"color: #008000;\">49.01703</span></td> \n   <td width=\"88\"><span style=\"color: #008000;\">8.7121</span></td> \n   <td width=\"88\"><span style=\"color: #008000;\">21.68871</span></td> \n  </tr> \n </tbody> \n</table> \n<p>Our fine-tuned model demonstrates remarkable efficiency, achieving about 22% overall improvement on the reasoning task after only one training epoch. The most significant gain appears in Rouge 2 scores—which measure bigram overlap—with about 49% increase, indicating better alignment between generated and reference summaries.</p> \n<p>Notably, preliminary experiments suggest these results could be further enhanced by extending the training duration. Increasing the number of epochs shows promising potential for additional performance gains while maintaining computational efficiency.</p> \n<h2>Clean up</h2> \n<p>To clean up your resources to avoid incurring any more charges, follow these steps:</p> \n<ol> \n <li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-jl-admin-guide-clean-up.html\" rel=\"noopener\" target=\"_blank\">Delete any unused SageMaker Studio resources</a></li> \n <li>(Optional) <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-delete-domain.html\" rel=\"noopener\" target=\"_blank\">Delete the SageMaker Studio domain</a></li> \n <li>Verify that your training job isn’t running anymore. To do so, on your SageMaker console, choose <strong>Training</strong> and check <strong>Training jobs</strong>.</li> \n <li>If you created a HyperPod cluster, <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-cli-command-delete-cluster.html\" rel=\"noopener\" target=\"_blank\">delete the cluster</a> to stop incurring costs. If you created the networking stack from the <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod/en-US/00-setup/02-own-account\" rel=\"noopener\" target=\"_blank\">HyperPod workshop</a>, delete the stack as well to clean up the virtual private cloud (VPC) resources and the FSx for Lustre volume.</li> \n</ol> \n<h2>Conclusion</h2> \n<p>In the first post of this two-part DeepSeek-R1 series, we discussed how SageMaker HyperPod recipes provide a powerful yet accessible solution for organizations to scale their AI model training capabilities with <a href=\"https://aws.amazon.com/what-is/large-language-model/\" rel=\"noopener\" target=\"_blank\">large language models</a> (LLMs) including DeepSeek. The architecture streamlines complex distributed training workflows through its intuitive recipe-based approach, reducing setup time from weeks to minutes.</p> \n<p>We recommend starting your LLM customization journey by exploring our sample recipes in the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-recipes.html\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker HyperPod</a> documentation. The AWS <a href=\"https://aws.amazon.com/training/learn-about/machine-learning/\" rel=\"noopener\" target=\"_blank\">AI/ML community</a> offers extensive resources, including workshops and technical guidance, to support your implementation journey.</p> \n<p>To begin using the SageMaker HyperPod recipes, visit the <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\" rel=\"noopener\" target=\"_blank\">sagemaker-hyperpod-recipes repo</a> on GitHub for comprehensive documentation and example implementations. Our team continues to expand the recipe ecosystem based on customer feedback and emerging ML trends, making sure that you have the tools needed for successful AI model training.</p> \n<p>In our second post, we discuss how these <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/fine-tuning/deepseek/hf_deepseek_r1_671b_seq8k_gpu_qlora.yaml\" rel=\"noopener\" target=\"_blank\">recipes</a> could further be used to fine-tune DeepSeek-R1 671b model. Stay tuned!</p> \n<hr style=\"width: 100%;\" /> \n<h3>About the Authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"wp-image-30789 size-full alignleft\" height=\"113\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/11/15/Kanwaljit-Khurmi-cropped.jpg\" width=\"100\" />&nbsp;Kanwaljit Khurmi</strong>&nbsp;is a Principal Worldwide Generative AI Solutions Architect at AWS. He collaborates with AWS product teams, engineering departments, and customers to provide guidance and technical assistance, helping them enhance the value of their hybrid machine learning solutions on AWS. Kanwaljit specializes in assisting customers with containerized applications and high-performance computing solutions.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"wp-image-26763 size-full alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/08/03/Bruno-Pistone.jpg\" width=\"100\" />&nbsp;Bruno Pistone</strong>&nbsp;is a Senior World Wide Generative AI/ML Specialist Solutions Architect at AWS based in Milan, Italy. He works with AWS product teams and large customers to help them fully understand their technical needs and design AI and Machine Learning solutions that take full advantage of the AWS cloud and Amazon Machine Learning stack. His expertise includes: End-to-end Machine Learning, model customization, and generative AI. He enjoys spending time with friends, exploring new places, and traveling to new destinations.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"wp-image-86104 size-full alignleft\" height=\"108\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/11/Arun.jpg\" width=\"100\" />&nbsp;Arun Kumar Lokanatha</strong> is a Senior ML Solutions Architect with the Amazon SageMaker team. He specializes in large language model training workloads, helping customers build LLM workloads using SageMaker HyperPod, SageMaker training jobs, and SageMaker distributed training. Outside of work, he enjoys running, hiking, and cooking.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"wp-image-29251 size-full alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/10/12/Durga-Sury.jpg\" width=\"100\" />&nbsp;Durga Sury</strong> is a Senior Solutions Architect on the Amazon SageMaker team. Over the past 5 years, she has worked with multiple enterprise customers to set up a secure, scalable AI/ML platform built on SageMaker.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"wp-image-94824 size-full alignleft\" height=\"101\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/28/Aman-2.jpg\" width=\"100\" />&nbsp;Aman Shanbhag</strong>&nbsp;is an Associate Specialist Solutions Architect on the ML Frameworks team at Amazon Web Services, where he helps customers and partners with deploying ML training and inference solutions at scale. Before joining AWS, Aman graduated from Rice University with degrees in computer science, mathematics, and entrepreneurship.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"wp-image-47720 size-full alignleft\" height=\"118\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/12/06/Anirudh-Viswanathan.png\" width=\"100\" />Anirudh Viswanathan</strong> is a Sr Product Manager, Technical – External Services with the SageMaker AI Training team. He holds a Masters in Robotics from Carnegie Mellon University, an MBA from the Wharton School of Business, and is named inventor on over 40 patents. He enjoys long-distance running, visiting art galleries, and Broadway shows.</p> ",
      "summary": "In this two-part series, we discuss how you can reduce the DeepSeek model customization complexity by using the pre-built fine-tuning workflows (also called “recipes”) for both DeepSeek-R1 model and its distilled variations, released as part of Amazon SageMaker HyperPod recipes. In this first post, we will build a solution architecture for fine-tuning DeepSeek-R1 distilled models and demonstrate the approach by providing a step-by-step example on customizing the DeepSeek-R1 Distill Qwen 7b model using recipes, achieving an average of 25% on all the Rouge scores, with a maximum of 49% on Rouge 2 score with both SageMaker HyperPod and SageMaker training jobs. The second part of the series will focus on fine-tuning the DeepSeek-R1 671b model itself.",
      "date_published": "2025-03-03T21:09:29+00:00",
      "author": {
        "name": "Kanwaljit Khurmi"
      }
    },
    {
      "id": "https://aws.amazon.com/blogs/machine-learning/reduce-conversational-ai-response-time-through-inference-at-the-edge-with-aws-local-zones/",
      "url": "https://aws.amazon.com/blogs/machine-learning/reduce-conversational-ai-response-time-through-inference-at-the-edge-with-aws-local-zones/",
      "title": "Reduce conversational AI response time through inference at the edge with AWS Local Zones",
      "content_html": "<p>Recent advances in <a href=\"https://aws.amazon.com/generative-ai/\" rel=\"noopener\" target=\"_blank\">generative AI</a> have led to the proliferation of new generation of <a href=\"https://aws.amazon.com/what-is/conversational-ai/\" rel=\"noopener\" target=\"_blank\">conversational AI</a> assistants powered by <a href=\"https://aws.amazon.com/what-is/foundation-models/\" rel=\"noopener\" target=\"_blank\">foundation models</a> (FMs). These latency-sensitive applications enable real-time text and voice interactions, responding naturally to human conversations. Their applications span a variety of sectors, including customer service, healthcare, education, personal and business productivity, and many others.</p> \n<p>Conversational AI assistants are typically deployed directly on users’ devices, such as smartphones, tablets, or desktop computers, enabling quick, local processing of voice or text input. However, the FM that powers the assistant’s natural language understanding and response generation is usually cloud-hosted, running on powerful GPUs. When a user interacts with the AI assistant, their device first processes the input locally, including speech-to-text (STT) conversion for voice agents, and compiles a prompt. This prompt is then securely transmitted to the cloud-based FM over the network. The FM analyzes the prompt and begins generating an appropriate response, streaming it back to the user’s device. The device further processes this response, including text-to-speech (TTS) conversion for voice agents, before presenting it to the user. This efficient workflow strikes a balance between the powerful capabilities of cloud-based FMs and the convenience and responsiveness of local device interaction, as illustrated in the following figure.<img alt=\"Request flow for a conversational AI assistant \" class=\"aligncenter wp-image-99743 size-full\" height=\"592\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/14/ML17594-image001-request-flow.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1181\" /></p> \n<p>A critical challenge in developing such applications is reducing response latency to enable real-time, natural interactions. Response latency refers to the time between the user finishing their speech and beginning to hear the AI assistant’s response. This delay typically comprises two primary components:</p> \n<ul> \n <li><strong>On-device processing latency</strong> – This encompasses the time required for local processing, including TTS and STT operations.</li> \n <li><strong>Time to first token (TTFT)</strong> – This measures the interval between the device sending a prompt to the cloud and receiving the first token of the response. TTFT consists of two components. First is the network latency, which is the round-trip time for data transmission between the device and the cloud. Second is the first token generation time, which is the period between the FM receiving a complete prompt and generating the first output token. TTFT is crucial for user experience in conversational AI interfaces that use response streaming with FMs. With response streaming, users start receiving the response while it’s still being generated, significantly improving perceived latency.</li> \n</ul> \n<p>The ideal response latency for humanlike conversation flow is generally considered to be in the 200–500 milliseconds (ms) range, closely mimicking natural pauses in human conversation. Given the additional on-device processing latency, achieving this target requires a TTFT well below 200 ms.</p> \n<p>Although many customers focus on optimizing the technology stack behind the FM inference endpoint through techniques such as <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-optimize.html\" rel=\"noopener\" target=\"_blank\">model optimization</a>, hardware acceleration, and <a href=\"https://aws.amazon.com/blogs/database/improve-speed-and-reduce-cost-for-generative-ai-workloads-with-a-persistent-semantic-cache-in-amazon-memorydb/\" rel=\"noopener\" target=\"_blank\">semantic caching</a> to reduce the TTFT, they often overlook the significant impact of <a href=\"https://aws.amazon.com/what-is/latency/\" rel=\"noopener\" target=\"_blank\">network latency</a>. This latency can vary considerably due to geographic distance between users and cloud services, as well as the diverse quality of internet connectivity.</p> \n<h2>Hybrid architecture with AWS Local Zones</h2> \n<p>To minimize the impact of network latency on TTFT for users regardless of their locations, a hybrid architecture can be implemented by extending AWS services from commercial <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\" rel=\"noopener\" target=\"_blank\">Regions</a> to edge locations closer to end users. This approach involves deploying additional inference endpoints on <a href=\"https://aws.amazon.com/edge/services/\" rel=\"noopener\" target=\"_blank\">AWS edge services</a> and using <a href=\"https://aws.amazon.com/route53/\" rel=\"noopener\" target=\"_blank\">Amazon Route 53</a> to implement dynamic <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\" rel=\"noopener\" target=\"_blank\">routing policies</a>, such as <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html\" rel=\"noopener\" target=\"_blank\">geolocation routing</a>, <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geoproximity.html\" rel=\"noopener\" target=\"_blank\">geoproximity routing</a>, or <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html\" rel=\"noopener\" target=\"_blank\">latency-based routing</a>. These strategies dynamically distribute traffic between edge locations and commercial Regions, providing fast response times based on real-time network conditions and user locations.</p> \n<p><a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/\" rel=\"noopener\" target=\"_blank\">AWS Local Zones</a> are a type of edge infrastructure deployment that places <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/features\" rel=\"noopener\" target=\"_blank\">select AWS services</a> close to large population and industry centers. They enable applications requiring very low latency or local data processing using familiar APIs and tool sets. Each Local Zone is a logical extension of a corresponding parent AWS Region, which means customers can <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/Extend_VPCs.html\" rel=\"noopener\" target=\"_blank\">extend</a> their <a href=\"https://aws.amazon.com/vpc/\" rel=\"noopener\" target=\"_blank\">Amazon Virtual Private Cloud (Amazon VPC)</a> connections by creating a new subnet with a Local Zone assignment.</p> \n<p>This guide demonstrates how to deploy an open source FM from <a href=\"https://huggingface.co/\" rel=\"noopener\" target=\"_blank\">Hugging Face</a> on <a href=\"https://aws.amazon.com/ec2/\" rel=\"noopener\" target=\"_blank\">Amazon Elastic Compute Cloud (Amazon EC2)</a> instances across three locations: a commercial AWS Region and two AWS Local Zones. Through comparative benchmarking tests, we illustrate how deploying FMs in Local Zones closer to end users can significantly reduce latency—a critical factor for real-time applications such as conversational AI assistants.</p> \n<h2>Prerequisites</h2> \n<p>To run this demo, complete the following prerequisites:</p> \n<ul> \n <li>Create an <a href=\"http://console.aws.amazon.com/\" rel=\"noopener\" target=\"_blank\">AWS account</a>, if you don’t already have one.</li> \n <li>Enable the Local Zones in Los Angeles and Honolulu in the parent Region US West (Oregon). For a full list of available Local Zones, refer to the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/locations/\" rel=\"noopener\" target=\"_blank\">Local Zones locations page</a>. Next, create a subnet inside each Local Zone. Detailed instructions for enabling Local Zones and creating subnets within them can be found at <a href=\"https://docs.aws.amazon.com/local-zones/latest/ug/getting-started.html\" rel=\"noopener\" target=\"_blank\">Getting started with AWS Local Zones</a>.</li> \n <li>Submit an <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html\" rel=\"noopener\" target=\"_blank\">Amazon EC2 service quota increase</a> for access to Amazon EC2 <a href=\"https://aws.amazon.com/ec2/instance-types/g4/\" rel=\"noopener\" target=\"_blank\">G4dn</a> instances. Select the <strong>Running On-Demand G and VT instances</strong> as the <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html#ec2-on-demand-instances-limits\" rel=\"noopener\" target=\"_blank\">quota type</a> and at least 24 vCPUs for the quota size.</li> \n <li>Create a Hugging Face read token from <a href=\"https://huggingface.co/settings/tokens\" rel=\"noopener\" target=\"_blank\">huggingface.co/settings/tokens</a>.</li> \n</ul> \n<h2>Solution walkthrough</h2> \n<p>This section walks you through the steps to launch an Amazon EC2 G4dn instance and deploy an FM for inference in the Los Angeles Local Zone. The instructions are also applicable for deployments in the parent Region, US West (Oregon), and the Honolulu Local Zone.</p> \n<p>We use Meta’s open source <a href=\"https://huggingface.co/meta-llama/Llama-3.2-3B\" rel=\"noopener\" target=\"_blank\">Llama 3.2-3B</a> as the FM for this demonstration. This is a lightweight FM from the Llama 3.2 family, classified as a small language model (SLM) due to its small number of parameters. Compared to <a href=\"https://aws.amazon.com/what-is/large-language-model/\" rel=\"noopener\" target=\"_blank\">large language models</a> (LLMs), SLMs are more efficient and cost-effective to train and deploy, excel when fine-tuned for specific tasks, offer faster inference times, and have lower resource requirements. These characteristics make SLMs particularly well-suited for deployment on edge services such as AWS Local Zones.</p> \n<p>To launch an EC2 instance in the Los Angeles Local Zone subnet, follow these steps:</p> \n<ol> \n <li>On the Amazon EC2 console dashboard, in the <strong>Launch instance</strong> box, choose <strong>Launch instance</strong>.</li> \n <li>Under <strong>Name and tags</strong>, enter a descriptive name for the instance (for example, <em>la-local-zone-instance</em>).</li> \n <li>Under <strong>Application and OS Images (Amazon Machine Image)</strong>, select an <a href=\"https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html\" rel=\"noopener\" target=\"_blank\">AWS Deep Learning AMI</a> that comes preconfigured with NVIDIA OSS driver and PyTorch. For our deployment, we used <strong>Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.3.1 (Amazon Linux 2)</strong>.</li> \n <li>Under <strong>Instance type</strong>, from the <strong>Instance type</strong> list, select the hardware configuration for your instance that’s supported in a Local Zone. We selected <code>G4dn.2xlarge</code> for this solution. This instance is equipped with one NVIDIA T4 Tensor Core GPU and 16 GB of GPU memory, which makes it ideal for high performance and cost-effective inference of SLMs on the edge. Available instance types for each Local Zone can be found at <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/features/\" rel=\"noopener\" target=\"_blank\">AWS Local Zones features</a>. Review the hardware requirements for your FM to select the appropriate instance.</li> \n <li>Under <strong>Key pair (login)</strong>, choose an existing key pair or create a new one.</li> \n <li>Next to <strong>Network settings</strong>, choose <strong>Edit</strong>, and then: \n  <ol type=\"a\"> \n   <li>Select your VPC.</li> \n   <li>Select your Local Zone subnet.</li> \n   <li>Create a security group or select an existing one. <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/working-with-security-group-rules.html\" rel=\"noopener\" target=\"_blank\">Configure</a> the security group’s inbound rules to allow traffic only from your client’s IP address on port 8080.</li> \n  </ol> </li> \n <li>You can keep the default selections for the other configuration settings for your instance. To determine the storage types that are supported, refer to the <strong>Compute and storage</strong> section in <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/features\" rel=\"noopener\" target=\"_blank\">AWS Local Zones features</a>.</li> \n <li>Review the summary of your instance configuration in the <strong>Summary</strong> panel and, when you’re ready, choose <strong>Launch instance</strong>.</li> \n <li>A confirmation page lets you know that your instance is launching. Choose <strong>View all instances</strong> to close the confirmation page and return to the console.</li> \n</ol> \n<p>Next, complete the following steps to deploy <a href=\"https://huggingface.co/meta-llama/Llama-3.2-3B\" rel=\"noopener\" target=\"_blank\">Llama 3.2-3B</a> using the Hugging Face <a href=\"https://huggingface.co/docs/text-generation-inference/en/index\" rel=\"noopener\" target=\"_blank\">Text Generation Inference (TGI)</a> as the model server:</p> \n<ol> \n <li>Connect by using Secure Shell (SSH) into the instance</li> \n <li>Start the docker service using the following command. This comes preinstalled with the AMI we selected.</li> \n</ol> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">sudo service docker start</code></pre> \n</div> \n<ol start=\"3\"> \n <li>Run the following command to download and run the Docker image for TGI server as well as Llama 3.2-3B model. In our deployment, we used Docker image version 2.4.0, but results might vary based on your selected version. The full list of supported models by TGI can be found at <a href=\"https://huggingface.co/docs/text-generation-inference/supported_models\" rel=\"noopener\" target=\"_blank\">Hugging Face Supported Models</a>. For more details about the deployment and optimization of TGI, refer to this <a href=\"https://github.com/huggingface/text-generation-inference\" rel=\"noopener\" target=\"_blank\">text-generation-inference</a> GitHub page.</li> \n</ol> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">model=meta-llama/Llama-3.2-3B\nvolume=$PWD/data\ntoken=&lt;ENTER YOUR HUGGING FACE TOKEN&gt;\n\nsudo docker run -d --gpus all \\\n    --shm-size 1g \\\n    -e HF_TOKEN=$token \\\n    -p 8080:80 \\\n    -v $volume:/data ghcr.io/huggingface/text-generation-inference:2.4.0 \\\n    --model-id $model</code></pre> \n</div> \n<ol start=\"4\"> \n <li>After the TGI container is running, you can test your endpoint by running the following command from your local environment:</li> \n</ol> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">curl &lt;REPLACE WITH YOUR EC2 PUBLIC IP &gt;:8080/generate -X POST \\\n    -d '{\"inputs\":\"What is deep learning?\",\"parameters\":{\"max_new_tokens\":200, \"temperature\":0.2, \"top_p\":0.9}}' \\\n    -H 'Content-Type: application/json'</code></pre> \n <h2>Performance evaluation</h2> \n <p>To demonstrate TTFT improvements with FM inference on Local Zones, we followed the steps in the previous section to deploy <a href=\"https://huggingface.co/meta-llama/Llama-3.2-3B\" rel=\"noopener\" target=\"_blank\">Llama 3.2 3B</a> in three locations: in the <code>us-west-2-c</code> Availability Zone in the parent Region, US West (Oregon); in the <code>us-west-2-lax-1a</code> Local Zone in Los Angeles; and in the <code>us-west-2-hnl-1a</code> Local Zone in Honolulu. This is illustrated in the following figure. Notice that the architecture provided in this post is meant to be used for performance evaluation in a development environment. Before migrating any of the provided architecture to production, we recommend following the <a href=\"https://aws.amazon.com/architecture/well-architected/\" rel=\"noopener\" target=\"_blank\">AWS Well-Architected Framework</a>.</p> \n <p>We conducted two separate test scenarios to evaluate TTFT as explained in the following:</p> \n <p>Los Angeles test scenario:</p> \n <ul> \n  <li><strong>Test user’s location</strong> – Los Angeles metropolitan area</li> \n  <li><strong>Test A</strong> – 150 requests sent to FM deployed in Los Angeles Local Zone</li> \n  <li><strong>Test B</strong> – 150 requests sent to FM deployed in US West (Oregon)</li> \n </ul> \n <p>Honolulu test scenario:</p> \n <ul> \n  <li><strong>Test user’s location</strong> – Honolulu metropolitan area</li> \n  <li><strong>Test C</strong> – 150 requests sent to FM deployed in Honolulu Local Zone</li> \n  <li><strong>Test D</strong> – 150 requests sent to FM deployed in US West (Oregon)</li> \n </ul> \n <p><img alt=\"Architecture diagram for the deployment of FM inference endpoints\" class=\"aligncenter wp-image-99718 size-full\" height=\"858\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/13/ML17594-002.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1286\" /></p> \n <h3>Evaluation setup</h3> \n <p>To conduct TTFT measurements, we use the load testing capabilities of the open source project <a href=\"https://github.com/ray-project/llmperf\" rel=\"noopener\" target=\"_blank\">LLMPerf</a>. This tool launches multiple requests from the test user’s client to the FM endpoint and measures various performance metrics, including TTFT. Each request contains a <a href=\"https://github.com/ray-project/llmperf?tab=readme-ov-file#load-test\" rel=\"noopener\" target=\"_blank\">random prompt</a> with a mean token count of 250 tokens. Although a single prompt for short-form conversations typically consists of 50 tokens, we set the mean input token size to 250 tokens to account for multi-turn conversation history, system prompts, and contextual information that better represents real-world usage patterns.</p> \n <p>Detailed instructions for installing LLMPerf and executing the load testing are available in the <a href=\"https://github.com/ray-project/llmperf\" rel=\"noopener\" target=\"_blank\">project’s documentation</a>. Additionally, because we are using the Hugging Face TGI as the inference server, we follow the corresponding <a href=\"https://github.com/ray-project/llmperf?tab=readme-ov-file#hugging-face\" rel=\"noopener\" target=\"_blank\">instructions</a> from LLMPerf to perform the load testing. The following is the example command to initiate the load testing from the command line:</p> \n <div class=\"hide-language\"> \n  <pre><code class=\"lang-bash\">export HUGGINGFACE_API_BASE=\"http://&lt;REPLACE WITH YOUR EC2 PUBLIC IP&gt;:8080\" \nexport HUGGINGFACE_API_KEY=\"\" \n\npython token_benchmark_ray.py \\\n&nbsp; &nbsp;&nbsp;--model \"huggingface/meta-llama/Llama-3.2-3B\" \\\n&nbsp; &nbsp;&nbsp;--mean-input-tokens 250 \\\n&nbsp; &nbsp;&nbsp;--stddev-input-tokens 50 \\\n&nbsp; &nbsp;&nbsp;--mean-output-tokens 100 \\\n&nbsp; &nbsp;&nbsp;--stddev-output-tokens 20 \\\n&nbsp; &nbsp;&nbsp;--max-num-completed-requests 150\\\n&nbsp; &nbsp;&nbsp;--timeout 600 \\\n&nbsp; &nbsp;&nbsp;--num-concurrent-requests 1 \\\n&nbsp; &nbsp;&nbsp;--results-dir \"result_outputs\" \\\n&nbsp; &nbsp;&nbsp;--llm-api \"litellm\" \\\n&nbsp; &nbsp;&nbsp;--additional-sampling-params '{}'\n</code></pre> \n </div> \n <p>Each test scenario compares the TTFT latency between Local Zone and the parent Region endpoints to assess the impact of geographical distance. Latency results might vary based on several factors, including:</p> \n <ul> \n  <li>Test parameters and configuration</li> \n  <li>Time of day and network traffic</li> \n  <li>Internet service provider</li> \n  <li>Specific client location within the test Region</li> \n  <li>Current server load</li> \n </ul> \n <h3>Results</h3> \n <p>The following tables below present TTFT measurements in milliseconds (ms) for two distinct test scenarios. The results demonstrate significant TTFT reductions when using a Local Zone compared to the parent Region for both the Los Angeles and the Honolulu test scenarios. The observed differences in TTFT are solely attributed to network latency because identical FM inference configurations were employed in both the Local Zone and the parent Region.</p> \n <table border=\"1px\" cellpadding=\"10px\" class=\"aligncenter\"> \n  <tbody> \n   <tr> \n    <td colspan=\"9\" style=\"background-color: #000000; text-align: center; vertical-align: middle;\"><span style=\"color: #ffffff;\">User location: Los Angeles Metropolitan Area</span></td> \n   </tr> \n   <tr> \n    <td width=\"179\">LLM inference endpoint</td> \n    <td width=\"73\">Mean (ms)</td> \n    <td>Min (ms)</td> \n    <td>P25 (ms)</td> \n    <td>P50 (ms)</td> \n    <td>P75 (ms)</td> \n    <td>P95 (ms)</td> \n    <td>P99 (ms)</td> \n    <td>Max (ms)</td> \n   </tr> \n   <tr> \n    <td width=\"179\">Parent Region: US West (Oregon)</td> \n    <td width=\"73\">135</td> \n    <td>118</td> \n    <td>125</td> \n    <td>130</td> \n    <td>139</td> \n    <td>165</td> \n    <td>197</td> \n    <td>288</td> \n   </tr> \n   <tr> \n    <td width=\"179\">Local Zone: Los Angeles</td> \n    <td width=\"73\">80</td> \n    <td>50</td> \n    <td>72</td> \n    <td>75</td> \n    <td>86</td> \n    <td>116</td> \n    <td>141</td> \n    <td>232</td> \n   </tr> \n  </tbody> \n </table> \n <p>The user in Los Angeles achieved a mean TTFT of 80 ms when calling the FM endpoint in the Los Angeles Local Zone, compared to 135 ms for the endpoint in the US West (Oregon) Region. This represents a 55 ms (about 41%) reduction in latency.</p> \n <table border=\"1px\" cellpadding=\"10px\" class=\"aligncenter\"> \n  <tbody> \n   <tr> \n    <td colspan=\"9\" style=\"background-color: #000000; text-align: center; vertical-align: middle;\"><span style=\"color: #ffffff;\">User location: Honolulu Metropolitan Area</span></td> \n   </tr> \n   <tr> \n    <td width=\"179\">LLM inference endpoint</td> \n    <td width=\"73\">Mean (ms)</td> \n    <td>Min (ms)</td> \n    <td>P25 (ms)</td> \n    <td>P50 (ms)</td> \n    <td>P75 (ms)</td> \n    <td>P95 (ms)</td> \n    <td>P99 (ms)</td> \n    <td>Max (ms)</td> \n   </tr> \n   <tr> \n    <td width=\"179\">Parent Region: US West (Oregon)</td> \n    <td width=\"73\">197</td> \n    <td>172</td> \n    <td>180</td> \n    <td>183</td> \n    <td>187</td> \n    <td>243</td> \n    <td>472</td> \n    <td>683</td> \n   </tr> \n   <tr> \n    <td width=\"179\">Local Zone: Honolulu</td> \n    <td width=\"73\">114</td> \n    <td>58</td> \n    <td>70</td> \n    <td>85</td> \n    <td>164</td> \n    <td>209</td> \n    <td>273</td> \n    <td>369</td> \n   </tr> \n  </tbody> \n </table> \n <p>The user in Honolulu achieved a mean TTFT of 114 ms when calling the FM endpoint in the Honolulu Local Zone, compared to 197 ms for the endpoint in the US West (Oregon) Region. This represents an 83 ms (about 42%) reduction in latency.</p> \n <p>Moreover, the TTFT reduction achieved by Local Zone deployments is consistent across all metrics in both test scenarios, from minimum to maximum values and throughout all percentiles (P25–P99), indicating a consistent improvement across all requests.</p> \n <p>Finally, remember that TTFT is just one component of overall response latency, alongside on-device processing latency. By reducing TTFT using Local Zones, you create additional margin for on-device processing latency, making it easier to achieve the target response latency range needed for humanlike conversation.</p> \n <h2>Cleanup</h2> \n <p>In this post, we created Local Zones, subnets, security groups, and EC2 instances. To avoid incurring additional charges, it’s crucial to properly clean up these resources when they’re no longer needed. To do so, follow these steps:</p> \n <ol> \n  <li>Terminate the EC2 instances and delete their associated <a href=\"https://aws.amazon.com/ebs/\" rel=\"noopener\" target=\"_blank\">Amazon Elastic Block Store</a> (Amazon EBS) volumes.</li> \n  <li>Delete the security groups and subnets.</li> \n  <li>Disable the Local Zones.</li> \n </ol> \n <h2>Conclusion</h2> \n <p>In conclusion, this post highlights how edge computing services, such as AWS Local Zones, play a crucial role in reducing FM inference latency for conversational AI applications. Our test deployments of Meta’s Llama 3.2-3B demonstrated that placing FM inference endpoints closer to end users through Local Zones dramatically reduces TTFT compared to traditional Regional deployments. This TTFT reduction plays a critical role in optimizing the overall response latency, helping achieve the target response times essential for natural, humanlike interactions regardless of user location.</p> \n <p>To use these benefits for your own applications, we encourage you to explore the AWS Local Zones documentation. There, you’ll find information on available <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/locations/\" rel=\"noopener\" target=\"_blank\">locations</a> and supported <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/features/\" rel=\"noopener\" target=\"_blank\">AWS services</a>&nbsp;so you can bring the power of edge computing to your conversational AI solutions.</p> \n <hr /> \n <h3>About the Authors</h3> \n <p style=\"clear: both;\"><strong><img alt=\"Nima Seifi\" class=\"size-full wp-image-99739 alignleft\" height=\"128\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/14/nimaseifi.jpeg\" width=\"100\" />Nima Seifi</strong> is a Solutions Architect at AWS, based in Southern California, where he specializes in SaaS and LLMOps. He serves as a technical advisor to startups building on AWS. Prior to AWS, he worked as a DevOps architect in the e-commerce industry for over 5 years, following a decade of R&amp;D work in mobile internet technologies. Nima has authored 20+ technical publications and holds 7 U.S. patents. Outside of work, he enjoys reading, watching documentaries, and taking beach walks.</p> \n <p style=\"clear: both;\"><strong><img alt=\"Nelson Ong\" class=\"size-full wp-image-99740 alignleft\" height=\"128\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/14/nelsonong.jpeg\" width=\"100\" />Nelson Ong</strong> is a Solutions Architect at Amazon Web Services. He works with early stage startups across industries to accelerate their cloud adoption.</p> \n</div> ",
      "summary": "This guide demonstrates how to deploy an open source foundation model from Hugging Face on Amazon EC2 instances across three locations: a commercial AWS Region and two AWS Local Zones. Through comparative benchmarking tests, we illustrate how deploying foundation models in Local Zones closer to end users can significantly reduce latency—a critical factor for real-time applications such as conversational AI assistants.",
      "date_published": "2025-03-03T16:44:46+00:00",
      "author": {
        "name": "Nima Seifi"
      }
    },
    {
      "id": "https://aws.amazon.com/blogs/machine-learning/pixtral-12b-2409-is-now-available-on-amazon-bedrock-marketplace/",
      "url": "https://aws.amazon.com/blogs/machine-learning/pixtral-12b-2409-is-now-available-on-amazon-bedrock-marketplace/",
      "title": "Pixtral-12B-2409 is now available on Amazon Bedrock Marketplace",
      "content_html": "<p>Today, we are excited to announce that Pixtral 12B (pixtral-12b-2409), a state-of-the-art 12 billion parameter vision language model (VLM) from <a href=\"https://mistral.ai/\" rel=\"noopener\" target=\"_blank\">Mistral AI</a> that excels in both text-only and multimodal tasks, is available for customers through <a href=\"https://aws.amazon.com/bedrock/marketplace/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock Marketplace</a>. Amazon Bedrock Marketplace is a new capability in <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock</a> that enables developers to discover, test, and use over 100 popular, emerging, and specialized foundation models (FMs) alongside the current selection of industry-leading models in Amazon Bedrock. You can also <a href=\"https://aws.amazon.com/blogs/machine-learning/pixtral-12b-is-now-available-on-amazon-sagemaker-jumpstart/\" rel=\"noopener\" target=\"_blank\">use this model with Amazon SageMaker JumpStart</a>, a machine learning (ML) hub that provides access to algorithms and models that can be deployed with one click for running inference.</p> \n<p>In this post, we walk through how to discover, deploy, and use the Pixtral 12B model for a variety of real-world vision use cases.</p> \n<h2>Overview of Pixtral 12B</h2> \n<p>Pixtral 12B, Mistral’s inaugural VLM, delivers robust performance across a range of benchmarks, surpassing other open models and rivaling larger counterparts, according to Mistral’s evaluation. Designed for both image and document comprehension, Pixtral demonstrates advanced capabilities in vision-related tasks, including chart and figure interpretation, document question answering, multimodal reasoning, and instruction following—several of which are illustrated with examples later in this post. The model processes images at their native resolution and aspect ratio, providing high-fidelity input handling. Unlike many open source alternatives, Pixtral 12B achieves strong results in text-based benchmarks—such as instruction following, coding, and mathematical reasoning—without sacrificing its proficiency in multimodal tasks.</p> \n<p>Mistral developed a novel architecture for Pixtral 12B, optimized for both computational efficiency and performance. The model consists of two main components: a 400-million-parameter vision encoder, responsible for tokenizing images, and a 12-billion-parameter multimodal transformer decoder, which predicts the next text token based on a sequence of text and images. The vision encoder was specifically trained to natively handle variable image sizes, enabling Pixtral to accurately interpret high-resolution diagrams, charts, and documents while maintaining fast inference speeds for smaller images such as icons, clipart, and equations. This architecture supports processing an arbitrary number of images of varying sizes within a large context window of 128k tokens.</p> \n<p>License agreements are a critical decision factor when using open-weights models. Similar to other Mistral models, such as Mistral 7B, Mixtral 8x7B, Mixtral 8x22B, and Mistral Nemo 12B, Pixtral 12B is released under the <a href=\"https://huggingface.co/mistralai/Pixtral-12B-2409\" rel=\"noopener\" target=\"_blank\">commercially permissive Apache 2.0</a>, providing enterprise and startup customers with a high-performing VLM option to build complex multimodal applications.</p> \n<h2>Performance metrics and benchmarks</h2> \n<p>Pixtral 12B is trained to understand both natural images and documents, achieving 52.5% on the Massive Multitask Language Understanding (MMLU) reasoning benchmark, surpassing a number of larger models according to Mistral. The MMLU benchmark is a test that evaluates a language model’s ability to understand and use language across a variety of subjects. The MMLU consists of over 10,000 multiple-choice questions spanning a variety of academic subjects, including mathematics, philosophy, law, and medicine. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning, and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process multiple images in its long context window of 128,000 tokens. Unlike previous open source models, Pixtral doesn’t compromise on text benchmark performance to excel in multimodal tasks, according to Mistral.</p> \n<p>You can review the <a href=\"https://mistral.ai/en/news/pixtral-12b\" rel=\"noopener\" target=\"_blank\">Mistral published benchmarks</a></p> \n<h2>Prerequisites</h2> \n<p>To try out Pixtral 12B in Amazon Bedrock Marketplace, you will need the following prerequisites:</p> \n<ul> \n <li>An AWS account that will contain all your AWS resources.</li> \n <li>An <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) role to access Amazon Bedrock Marketplace and <a href=\"https://aws.amazon.com/sagemaker/\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker</a> endpoints. To learn more about how IAM works with Amazon Bedrock Marketplace, refer to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/setup-amazon-bedrock-marketplace.html\" rel=\"noopener\" target=\"_blank\">Set up Amazon Bedrock Marketplace</a>.</li> \n <li>Access to accelerated instances (GPUs) for hosting the model, such as ml.g6.12xlarge. Refer to <a href=\"https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html\" rel=\"noopener\" target=\"_blank\">Requesting a quota increase</a> for access to GPU instances.</li> \n</ul> \n<h2>Deploy Pixtral 12B in Amazon Bedrock Marketplace</h2> \n<p>On the Amazon Bedrock console, you can search for models that help you with a specific use case or language. The results of the search include both serverless models and models available in Amazon Bedrock Marketplace. You can filter results by provider, modality (such as text, image, or audio), or task (such as classification or text summarization).</p> \n<p>To access Pixtral 12B in Amazon Bedrock Marketplace, follow these steps:</p> \n<ol> \n <li>On the Amazon Bedrock console, choose <b>Model catalog</b> under <b>Foundation models</b> in the navigation pane.</li> \n <li>Filter for Hugging Face as a provider and choose the Pixtral 12B model, or search for Pixtral in the <b>Filter for a model</b> input box.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-100457\" height=\"1048\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image01.png\" width=\"1571\" /></li> \n</ol> \n<p>The model detail page provides essential information about the model’s capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration.</p> \n<p>The page also includes deployment options and licensing information to help you get started with Pixtral 12B in your applications.</p> \n<ol start=\"3\"> \n <li>To begin using Pixtral 12B, choose <b>Deploy</b>.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-100456\" height=\"766\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image02.png\" width=\"1273\" /></li> \n</ol> \n<p>You will be prompted to configure the deployment details for Pixtral 12B. The model ID will be prepopulated.</p> \n<ol start=\"4\"> \n <li>Read carefully and accept the End User License Agreement (EULA).</li> \n <li>The <strong>Endpoint Name </strong>is automatically populated. Customers can choose to rename the endpoint.</li> \n <li>For <b>Number of instances</b>, enter a number of instances (between 1–100).</li> \n <li>For <b>Instance type</b>, choose your instance type. For optimal performance with Pixtral 12B, a GPU-based instance type like ml.g6.12xlarge is recommended.</li> \n</ol> \n<p>Optionally, you can configure advanced security and infrastructure settings, including virtual private cloud (VPC) networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, you might want to review these settings to align with your organization’s security and compliance requirements.</p> \n<ol start=\"8\"> \n <li>Choose <b>Deploy</b> to begin using the model.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-100455\" height=\"1093\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image03.png\" width=\"1544\" /></li> \n</ol> \n<p>When the deployment is complete, <b>Endpoint status</b> should change to <b>In Service</b><i>. </i>After the endpoint is in service, you can test Pixtral 12B capabilities directly in the Amazon Bedrock playground.</p> \n<ol start=\"9\"> \n <li>Choose <b>Open in playground</b> to access an interactive interface where you can experiment with different prompts and adjust model parameters like temperature and maximum length.</li> \n</ol> \n<p>This is an excellent way to explore the model’s reasoning and text generation abilities before integrating it into your applications. The playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-100454\" height=\"1121\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image04.png\" width=\"1109\" /></p> \n<p>You can quickly test the model in the playground through the UI. However, to invoke the deployed model programmatically with Amazon Bedrock APIs, you need to use the endpoint ARN as <code>model-id</code> in the Amazon Bedrock SDK.</p> \n<h2>Pixtral 12B use cases</h2> \n<p>In this section, we provide example use cases of Pixtral 12B using sample prompts. We have defined helper functions to invoke the Pixtral 12B model using Amazon Bedrock Converse APIs:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">def get_image_format(image_path):\n    with Image.open(image_path) as img:\n        # Normalize the format to a known valid one\n        fmt = img.format.lower() if img.format else 'jpeg'\n        # Convert 'jpg' to 'jpeg'\n        if fmt == 'jpg':\n            fmt = 'jpeg'\n    return fmt\n\ndef call_bedrock_model(model_id=None, prompt=\"\", image_paths=None, system_prompt=\"\", temperature=0.6, top_p=0.9, max_tokens=3000):\n    \n    if isinstance(image_paths, str):\n        image_paths = [image_paths]\n    if image_paths is None:\n        image_paths = []\n    \n    # Start building the content array for the user message\n    content_blocks = []\n\n    # Include a text block if prompt is provided\n    if prompt.strip():\n        content_blocks.append({\"text\": prompt})\n\n    # Add images as raw bytes\n    for img_path in image_paths:\n        fmt = get_image_format(img_path)\n        # Read the raw bytes of the image (no base64 encoding!)\n        with open(img_path, 'rb') as f:\n            image_raw_bytes = f.read()\n\n        content_blocks.append({\n            \"image\": {\n                \"format\": fmt,\n                \"source\": {\n                    \"bytes\": image_raw_bytes\n                }\n            }\n        })\n\n    # Construct the messages structure\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": content_blocks\n        }\n    ]\n\n    # Prepare additional kwargs if system prompts are provided\n    kwargs = {}\n    \n    kwargs[\"system\"] = [{\"text\": system_prompt}]\n\n    # Build the arguments for the `converse` call\n    converse_kwargs = {\n        \"messages\": messages,\n        \"inferenceConfig\": {\n            \"maxTokens\": 4000,\n            \"temperature\": temperature,\n            \"topP\": top_p\n        },\n        **kwargs\n    }\n\n    \n    converse_kwargs[\"modelId\"] = model_id\n\n    # Call the converse API\n    try:\n        response = bedrock_runtime.converse(**converse_kwargs)\n    \n        # Parse the assistant response\n        assistant_message = response.get('output', {}).get('message', {})\n        assistant_content = assistant_message.get('content', [])\n        result_text = \"\".join(block.get('text', '') for block in assistant_content)\n    except Exception as e:\n        result_text = f\"Error message: {e}\"\n    return result_text</code></pre> \n</div> \n<h3>Visual logical reasoning</h3> \n<p>One of the interesting use cases of vision models is solving logical reasoning problems or visual puzzles. Pixtral 12B vision models are highly capable in solving logical reasoning questions. Let’s explore an example.</p> \n<p>We use the following input image.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-100453\" height=\"530\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image05.png\" width=\"1580\" /></p> \n<p>Our prompt and input payload are as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">system_prompt='You are solving logical reasoning problems.'\ntask = 'Which of these figures differ from the other four?'\nimage_path = './Pixtral_data/logical_reasoning.jpg'\n\nprint('Input Image:\\n\\n')\nImage.open(image_path).show()\n\nresponse = call_bedrock_model(model_id=endpoint_arn, \n                   prompt=task, \n                   system_prompt=system_prompt,\n                   image_paths = image_path)\n\nprint(f'\\nResponse from the model:\\n\\n{response}')</code></pre> \n</div> \n<p>We get following response:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">The image shows five figures labeled A, B, C, D, and E. Each figure consists of a head, body, and legs. The task is to identify which figure differs from the others.\n\n**Reasoning:**\n- Figure A: The head is connected directly to the body, and the legs are straight.\n- Figure B: The head is connected directly to the body, and the legs are straight.\n- Figure C: The head is connected directly to the body, and the legs are straight.\n- Figure D: The head is connected directly to the body, and the legs are straight.\n- Figure E: The head is connected directly to the body, but the legs are bent at the knees.\n\n**Conclusion:** Figure E is the one that differs from the others.</code></pre> \n</div> \n<h3>Structured product information</h3> \n<p>Extracting product information is crucial for the retail industry, especially on sites that host third-party sellers, where product images are the most accessible resource. Accurately capturing relevant details from these images is vital for a product’s success in ecommerce. For instance, using advanced visual models like Pixtral 12B, retailers can efficiently extract key attributes from clothing product images, such as color, style, and patterns. This capability not only streamlines inventory management but also enhances customer experiences by providing essential information that aids in informed purchasing decisions.</p> \n<p>We use the following input image.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-100452 size-medium\" height=\"284\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image06-300x284.png\" width=\"300\" /></p> \n<p>Our prompt and input payload are as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">system_prompt='''You are a catalog manager for an ecommerce portal. You have an image of a product. \nThink very carefully in a step-by-step manner to extract product information. \nAlways provide response in prescribed JSON format.\n'''\nimage_path = './Pixtral_data/cap.png'\ntask = '''\nOrganize product information in JSON format to store in a database. \n\nOutput json schema:\n\n{\n\"product_name\": \"\",\n\"description\": \"\",\n\"category\": \"\",\n\"sub_category\": \"\",\n\"color\": \"\",\n\"size\": \"\",\n\"brand\": \"\",\n\"material\": \"\",\n\"features\": [],\n\"image_alt_text\": \"\"\n}\n'''\n\nprint('Input Image:\\n\\n')\nImage.open(image_path).show()\n\nresponse = call_bedrock_model(model_id=endpoint_arn, \n                   prompt=task, \n                   system_prompt=system_prompt,\n                   image_paths = image_path)\n\nprint(f'\\nResponse from the model:\\n\\n{response}')</code></pre> \n</div> \n<p>We get the following response:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">Certainly! Based on the image provided, here is the extracted product information in JSON format:\n\n```json\n{\n  \"product_name\": \"Nike Blue Cap\",\n  \"description\": \"A stylish blue cap from Nike with a sleek design and adjustable fit.\",\n  \"category\": \"Apparel\",\n  \"sub_category\": \"Headwear\",\n  \"color\": \"Blue\",\n  \"size\": \"Adjustable\",\n  \"brand\": \"Nike\",\n  \"material\": \"Nylon\",\n  \"features\": [\n    \"Adjustable fit\",\n    \"Lightweight\",\n    \"Breathable material\",\n    \"Embroidered Nike logo\"\n  ],\n  \"image_alt_text\": \"A blue Nike cap with a white Nike swoosh logo on the front.\"\n}\n```</code></pre> \n</div> \n<h3>Vehicle damage assessment</h3> \n<p>In the insurance industry, image analysis plays a crucial role in claims processing. For vehicle damage assessment, vision models like Pixtral 12B can be used to compare images taken at policy issuance with those submitted during a claim. This approach can streamline the evaluation process, potentially reducing loss adjustment expenses and expediting claim resolution. By automating the identification and characterization of automobile damage, insurers can enhance efficiency, improve accuracy, and ultimately provide a better experience for policyholders.</p> \n<p>We use the following input images.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-100451\" height=\"450\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image07.png\" width=\"1205\" /></p> \n<p>Our prompt and input payload are as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">image_paths = [\n    \"Pixtral_data/car_image_before.png\",\n    \"Pixtral_data/car_image_after.png\"\n]\n\nsystem_prompt='''\nYou are a helpful ai assistant for an insurance agent. \nInsurance agent has received a claim for a vehicle damage. \n'''\n\ntask = '''This claim includes two images. \nOne of the image was taken before the incident and another was taken after the incident.\n\nAnalyse these images and answer below questions:\n1. describe if there is any damage to the vehicle\n2. should insurance agent accept or reject the claim\n\n'''\n\nprint('Input Images:\\n\\n')\nImage.open(image_paths[0]).show()\nImage.open(image_paths[1]).show()\n\nresponse = call_bedrock_model(model_id=endpoint_arn, \n                   prompt=task, \n                   system_prompt=system_prompt,\n                   image_paths = image_paths)\n\nprint(f'\\nResponse from the model:\\n\\n{response}')</code></pre> \n</div> \n<p>We get the following response:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">### Analysis of Vehicle Damage\n\n#### Image 1 (Before Incident)\n- **Vehicle Condition**: The vehicle appears to be in good condition with no visible damage.\n- **Paint and Body**: The paint is uniform, and there are no visible scratches, dents, or other forms of damage.\n- **Tires and Wheels**: The tires and wheels look intact and in good condition.\n\n#### Image 2 (After Incident)\n- **Vehicle Condition**: The vehicle shows noticeable damage.\n- **Paint and Body**: There is a visible dent or scratch on the rear side of the vehicle.\n- **Tires and Wheels**: The tires and wheels appear to be intact, but the focus is on the body damage.\n\n### Questions and Answers\n\n1. **Describe if there is any damage to the vehicle:**\n   - **Before Incident**: No visible damage.\n   - **After Incident**: There is visible damage on the rear side of the vehicle, likely a dent or scratch.\n\n2. **Should the insurance agent accept or reject the claim:**\n   - **Recommendation**: The insurance agent should accept the claim. The images clearly show that the vehicle sustained damage after the incident. The before and after images provide sufficient evidence of the damage, which was likely caused by an incident.\n\n### Conclusion\nBased on the analysis of the images, the insurance agent should accept the claim as the damage to the vehicle is evident and appears to have occurred after the incident.</code></pre> \n</div> \n<h3>Handwriting recognition</h3> \n<p>Another feature in vision language models is their ability to recognize handwriting and extract handwritten text. Pixtral 12B performs well on extracting content from complex and poorly handwritten notes.</p> \n<p>We use the following input image.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-100450\" height=\"105\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image08.png\" width=\"1870\" /><br /> Our prompt and input payload are as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">system_prompt='You are a Graphologists'\ntask = '''\nAnalyze the image and transcribe any handwritten text present. \nConvert the handwriting into a single, continuous string of text. \nMaintain the original spelling, punctuation, and capitalization as written. Ignore any printed text, drawings, or other non-handwritten elements in the image.\n'''\n\nimage_path = './Pixtral_data/a01-000u-04.png'\n\n\nprint('Input Image:\\n\\n')\nImage.open(image_path).show()\n\nresponse = call_bedrock_model(model_id=endpoint_arn, \n                   prompt=task, \n                   system_prompt=system_prompt,\n                   image_paths = image_path)\n\nprint(f'\\nResponse from the model:\\n\\n{response}')</code></pre> \n</div> \n<p>We get the following response:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">put down a resolution on the subject</code></pre> \n</div> \n<h3>Reasoning of complex figures</h3> \n<p>VLMs excel at interpreting and reasoning about complex figures, charts, and diagrams. In this particular use case, we use Pixtral 12B to analyze an intricate image containing GDP data. Pixtral 12B’s advanced capabilities in document understanding and complex figure analysis make it well-suited for extracting insights from visual representations of economic data. By processing both the visual elements and accompanying text, Pixtral 12B can provide detailed interpretations and reasoned analysis of the GDP figures presented in the image.</p> \n<p>We use the following input image.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-100449\" height=\"1330\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image09.png\" width=\"1530\" /></p> \n<p>Our prompt and input payload are as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">system_prompt='You are a Global Economist.'\ntask = 'List the top 5 countries in Europe with the highest GDP'\nimage_path = './Pixtral_data/gdp.png'\n\nprint('Input Image:\\n\\n')\nImage.open(image_path).show()\n\nresponse = call_bedrock_model(model_id=endpoint_arn, \n                   prompt=task, \n                   system_prompt=system_prompt,\n                   image_paths = image_path)\n\nprint(f'\\nResponse from the model:\\n\\n{response}')</code></pre> \n</div> \n<p>We get the following response:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">The top 5 countries in Europe with the highest GDP, based on the provided diagram, are:\n\n1. **Germany**\n   - GDP: $3.99 trillion\n   - GDP Percentage: 4.65%\n\n2. **United Kingdom**\n   - GDP: $2.82 trillion\n   - GDP Percentage: 3.29%\n\n3. **France**\n   - GDP: $2.78 trillion\n   - GDP Percentage: 3.24%\n\n4. **Italy**\n   - GDP: $2.07 trillion\n   - GDP Percentage: 2.42%\n\n5. **Spain**\n   - GDP: $1.43 trillion\n   - GDP Percentage: 1.66%\n\nThese countries are highlighted in green on the diagram.</code></pre> \n</div> \n<h2>Clean up</h2> \n<p>To avoid unwanted charges, clean up your resources. If you deployed the model using Amazon Bedrock Marketplace, complete the following steps:</p> \n<h3>Delete the Amazon Bedrock Marketplace deployment</h3> \n<ol> \n <li>On the Amazon Bedrock console, under <b>Foundation models</b> in the navigation pane, choose <b>Marketplace deployments</b>.</li> \n <li>In the <b>Managed deployments</b> section, locate the endpoint you want to delete.</li> \n <li>Verify the endpoint details to make sure you’re deleting the correct deployment: \n  <ol> \n   <li>Endpoint name</li> \n   <li>Model name</li> \n   <li>Endpoint status</li> \n  </ol> </li> \n <li>Select the endpoint, and choose <b>Delete</b>.</li> \n <li>Choose <b>Delete</b> to delete the endpoint.</li> \n <li>In the deletion confirmation dialog, review the warning message, enter <code>confirm</code>, and choose <b>Delete</b> to permanently remove the endpoint.</li> \n</ol> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-100448\" height=\"692\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/ml-18347-image10.png\" width=\"1172\" /></p> \n<h2>Conclusion</h2> \n<p>In this post, we showed you how to get started with the Pixtral 12B model in Amazon Bedrock and deploy the model for inference. The Pixtral 12B vision model enables you to solve multiple use cases, including document understanding, logical reasoning, handwriting recognition, image comparison, entity extraction, extraction of structured data from scanned images, and caption generation. These capabilities can drive productivity in a number of enterprise use cases, including ecommerce (retail), marketing, FSI, and much more.</p> \n<p>For more Mistral resources on AWS, check out the <a href=\"https://github.com/aws-samples/mistral-on-aws\" rel=\"noopener\" target=\"_blank\">GitHub repo</a>. The complete code for the samples featured in this post is available on GitHub. Pixtral 12B is also available in <a href=\"https://aws.amazon.com/sagemaker/jumpstart/\" rel=\"noopener\" target=\"_blank\">Amazon SageMaker JumpStart</a>; refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/pixtral-12b-is-now-available-on-amazon-sagemaker-jumpstart/\" rel=\"noopener\" target=\"_blank\">Pixtral 12B is now available on Amazon SageMaker JumpStart</a> for details.</p> \n<hr style=\"width: 100%;\" /> \n<h3>About the Authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-100463 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/dhapola.jpeg\" width=\"100\" />Deepesh Dhapola</strong> is a Senior Solutions Architect at AWS India, where he assists financial services and fintech clients in scaling and optimizing their applications on the AWS platform. He specializes in core machine learning and generative AI. Outside of work, Deepesh enjoys spending time with his family and experimenting with various cuisines.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-76504 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/05/17/tuggle.jpeg\" width=\"100\" />Preston Tuggle</strong> is a Sr. Specialist Solutions Architect working on generative AI.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-57983 alignleft\" height=\"132\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/06/16/shane-rane.png\" width=\"100\" />Shane Rai</strong> is a Principal GenAI Specialist with the AWS World Wide Specialist Organization (WWSO). He works with customers across industries to solve their most pressing and innovative business needs using AWS’s breadth of cloud-based AI/ML services including model offerings from top tier foundation model providers.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-100461 alignleft\" height=\"110\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/25/john.png\" width=\"100\" />John Liu</strong> has 14 years of experience as a product executive and 10 years of experience as a portfolio manager. At AWS, John is a Principal Product Manager for Amazon Bedrock. Previously, he was the Head of Product for AWS Web3 / Blockchain. Prior to AWS, John held various product leadership roles at public blockchain protocols and fintech companies, and also spent 9 years as a portfolio manager at various hedge funds.</p> ",
      "summary": "In this post, we walk through how to discover, deploy, and use the Mistral AI Pixtral 12B model for a variety of real-world vision use cases.",
      "date_published": "2025-03-03T16:43:08+00:00",
      "author": {
        "name": "Deepesh Dhapola"
      }
    },
    {
      "id": "https://blogs.nvidia.com/blog/ai-protects-wildlife/",
      "url": "https://blogs.nvidia.com/blog/ai-protects-wildlife/",
      "title": "Animals Crossing: AI Helps Protect Wildlife Across the Globe",
      "content_html": "<div id=\"bsf_rt_marker\"></div><p>From Seattle, Washington, to Cape Town, South Africa — and everywhere around and between — AI is helping conserve the wild plants and animals that make up the intricate web of life on Earth.</p>\n<p>It’s critical work that sustains ecosystems and supports biodiversity at a time when the United Nations <a href=\"https://wildlifeday.org/en/about\" target=\"_blank\">estimates</a> over 1 million species are threatened with extinction.</p>\n<p>World Wildlife Day, a UN initiative, is celebrated every March 3 to recognize the unique contributions wild animals and plants have on people and the planet — and vice versa.</p>\n<p></p>\n<p>“Our own survival depends on wildlife,” the above video on this year’s celebration says, “just as much as their survival depends on us.”</p>\n<p>Learn more about some of the leading nonprofits and startups using <a href=\"https://www.nvidia.com/en-us/solutions/ai/\" target=\"_blank\">NVIDIA AI</a> and <a href=\"https://www.nvidia.com/en-us/data-center/solutions/accelerated-computing/\" target=\"_blank\">accelerated computing</a> to protect wildlife and natural habitats, today and every day:</p>\n<h2><b>Ai2’s EarthRanger</b><b> Offers World’s Largest Elephant Database</b></h2>\n<p>Seattle-based nonprofit AI research institute Ai2 offers EarthRanger, a software platform that helps protected-area managers, ecologists and wildlife biologists make more informed operational decisions for wildlife conservation in real time, whether preventing poaching, spotting ill or injured animals, or studying animal behavior.</p>\n<p></p>\n<p>Among Ai2’s efforts with EarthRanger is the planned development of a machine learning model — trained using NVIDIA Hopper GPUs in the cloud — that predicts the movement of elephants in areas close to human-wildlife boundaries where elephants could raid crops and potentially prompt humans to retaliate.</p>\n<p>With access to the world’s largest repository of elephant movement data, made possible by EarthRanger users who’ve shared their data, the AI model could help predict elephant behaviors, then alert area managers to safely guide the elephants away from risky situations that could arise for them or for people in the vicinity. Area managers or rangers typically use helicopters, other vehicles and <a href=\"https://www.worldwildlife.org/magazine/issues/summer-2016/articles/using-chili-bombs-to-protect-both-elephants-and-farmers\" target=\"_blank\">chili bombs</a> to safely reroute elephants.</p>\n<figure class=\"wp-caption aligncenter\" id=\"attachment_78108\" style=\"width: 960px;\"><img alt=\"\" class=\"size-medium wp-image-78108\" height=\"640\" src=\"https://blogs.nvidia.com/wp-content/uploads/2025/03/earthranger-elephant-hugo-960x640.jpg\" width=\"960\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-78108\">An elephant named Hugo wears a monitoring device that helps keep him safe. Image courtesy of the Mara Elephant Project.</figcaption></figure>\n<p>Beyond elephants, EarthRanger collects, integrates and displays data on a slew of wildlife — aggregated from over 100 data sources, including camera traps, acoustic sensors, satellites, radios and more. Then, the platform combines the data with field reports to provide a unified view of collared wildlife, rangers, enforcement assets and infrastructure within a protected area.</p>\n<figure class=\"wp-caption aligncenter\" id=\"attachment_78111\" style=\"width: 512px;\"><img alt=\"\" class=\"size-full wp-image-78111\" height=\"300\" src=\"https://blogs.nvidia.com/wp-content/uploads/2025/03/earthranger-interface.png\" width=\"512\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-78111\">EarthRanger platform interface.</figcaption></figure>\n<p>“Name a country, species or an environmental cause and we’re probably supporting a field organization’s conservation efforts there,” said Jes Lefcourt, director of EarthRanger at Ai2.</p>\n<p>It’s deployed by governments and conservation organizations in 76 countries and 650 protected areas, including nearly every national park in Africa, about a dozen state fishing and wildlife departments in the U.S., as well as many other users across Latin America and Asia.</p>\n<p>Four of these partners — Rouxcel Technology, OroraTech, Wildlife Protection Services and Conservation X Labs — are highlighted below.</p>\n<h2><b>Rouxcel Technology</b><b> Saves Rhinos With AI</b></h2>\n<p>South African startup Rouxcel Technology’s AI-based RhinoWatches, tapping into EarthRanger, learn endangered black and white rhinos’ behaviors, then alert authorities in real time of any detected abnormalities. These abnormalities can include straying from typical habitats, territorial fighting with other animals and other potentially life-threatening situations.</p>\n<p>It’s critical work, as there are just about <a href=\"https://apnews.com/article/rhino-poaching-horns-preservation-52691fcd04e52f7cdf0a7c2eb369d9a8\" target=\"_blank\">28,000 rhinos left in the world</a>, from 500,000 at the beginning of the 20th century.</p>\n<figure class=\"wp-caption aligncenter\" id=\"attachment_78114\" style=\"width: 960px;\"><img alt=\"\" class=\"size-medium wp-image-78114\" height=\"1200\" src=\"https://blogs.nvidia.com/wp-content/uploads/2025/03/rouxcel-rhinowatch-960x1200.jpg\" width=\"960\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-78114\">A white rhino sports a Rouxcel RhinoWatch. Image courtesy of Hannah Rippon.</figcaption></figure>\n<p>Rouxcel, based in Cape Town, has deployed over 1,200 RhinoWatches — trained and optimized using NVIDIA accelerated computing — across more than 40 South African reserves. The startup, which uses the Ai2 EarthRanger platform, protects more than 1.2 million acres of rhino habitats, and has recently expanded to help conservation efforts in Kenya and Namibia.</p>\n<p>Looking forward, Rouxcel is developing AI models to help prevent poaching and human-wildlife conflict for more species, including pangolins, a critically endangered species.</p>\n<h2><b>OroraTech</b><b> Monitors Wildfires and Poaching With NVIDIA CUDA, Jetson</b></h2>\n<p>OroraTech — a member of the <a href=\"https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33\" target=\"_blank\">NVIDIA Inception</a> program for cutting-edge startups — uses the EarthRanger platform to protect wildlife in a different way, offering a wildfire detection and monitoring service that fuses satellite imagery and AI to safeguard the environment and prevent poaching.</p>\n<p></p>\n<p><a href=\"https://blogs.nvidia.com/blog/ororatech-wildfires-from-space/\">Combining data</a> from satellites, ground-based cameras, aerial observations and local weather information, OroraTech detects threats to natural habitats and alerts users in real time. The company’s technologies monitor more than 30 million hectares of land that directly impact wildlife in Africa and Australia. That’s nearly the size of the Great Barrier Reef.</p>\n<figure class=\"wp-caption aligncenter\" id=\"attachment_78117\" style=\"width: 960px;\"><img alt=\"\" class=\"size-medium wp-image-78117\" height=\"960\" src=\"https://blogs.nvidia.com/wp-content/uploads/2025/03/ororatech-bushfire-960x960.png\" width=\"960\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-78117\">OroraTech detects an early bushfire near Expedition National Park in Australia.</figcaption></figure>\n<p>OroraTech flies an <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/\" target=\"_blank\">NVIDIA Jetson</a> module for edge AI and data processing onboard all of its satellite payloads — the instruments, equipment and systems on a satellite designed for performing specific tasks. Through GPU-accelerated image processing, OroraTech achieves exceptional latency, delivering fire notifications to users on the ground as fast as five minutes after image acquisition.</p>\n<p>The AI-based fire-detection pipeline uses the <a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\">NVIDIA cuDNN</a> library of deep neural network primitives and the <a href=\"https://developer.nvidia.com/tensorrt\" target=\"_blank\">NVIDIA TensorRT</a> software development kit for thermal anomaly detection and cloud masking in space, leading to high-precision fire detections.</p>\n<h2><b>Wildlife Protection Solutions </b><b>Help Preserve Endangered Species</b></h2>\n<p>International nonprofit Wildlife Protection Solutions (WPS) supports more than 250 conservation projects in 50+ countries. Its remote cameras — about 3,000 deployed across the globe — using AI models provide real-time monitoring of animals and poachers, alerting rangers to intercede before wildlife is harmed.</p>\n<figure class=\"wp-caption aligncenter\" id=\"attachment_78120\" style=\"width: 960px;\"><img alt=\"\" class=\"wp-image-78120 size-medium\" height=\"691\" src=\"https://blogs.nvidia.com/wp-content/uploads/2025/03/wps-lion-960x691.png\" width=\"960\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-78120\">A lion detected with WPS technologies.</figcaption></figure>\n<p>WPS — which also taps into the EarthRanger platform — harnesses NVIDIA accelerated computing to optimize training and inference of its AI models, which process and analyze 65,000 photos per day.</p>\n<p></p>\n<p>The <a href=\"https://www.wildlifeprotectionsolutions.org/conservation/#remote%20monitoring\" target=\"_blank\">WPS tool</a> is free and available on any mobile, tablet or desktop browser, enabling remote monitoring, early alerting and proactive, automated deterrence of wildlife or humans in sensitive areas.</p>\n<h2><b>Conservation X Labs</b><b> Identifies Species From Crowdsourced Images</b></h2>\n<p>Seattle-based Conservation X Labs — which is on a mission to prevent the <a href=\"https://www.worldwildlife.org/stories/what-is-the-sixth-mass-extinction-and-what-can-we-do-about-it\" target=\"_blank\">sixth mass extinction</a>, or the dying out of a high percentage of the world’s biodiversity due to natural phenomena and human activity — also uses EarthRanger, including for its <a href=\"https://conservationxlabs.com/wild-me\" target=\"_blank\">Wild Me</a> solution: open-source AI software for the conservation research community.</p>\n<p>Wild Me supports over 2,000 researchers across the globe running AI-enabled wildlife population studies for marine and terrestrial species.</p>\n<p>In the below video, Wild Me helps researchers classify whale sharks using computer vision:</p>\n<p></p>\n<p>The crowdsourced database — which currently comprises 14 million photos — lets anyone upload imagery of species. Then, AI <a href=\"https://blogs.nvidia.com/blog/what-are-foundation-models/\">foundation models</a> trained using NVIDIA accelerated computing help identify species to ease and accelerate animal population assessments and other research that supports the fight against species extinction.</p>\n<p>In addition, Conservation X Labs’s <a href=\"https://conservationxlabs.com/the-sentinel\" target=\"_blank\">Sentinel</a> technology transforms traditional wildlife monitoring tools — like trail cameras and acoustic recorders — with AI, processing environmental data as it’s collected and providing conservationists with real-time, data-driven insights through satellite and cellular networks.</p>\n<p>To date, Sentinel devices have delivered about 100,000 actionable insights for 80 different species. For example, see how the technology flags a limping panther, so wildlife protectors could rapidly step in to offer aid:</p>\n<div class=\"wp-video\" style=\"width: 1280px;\"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->\n<video class=\"wp-video-shortcode\" controls=\"controls\" height=\"1000\" id=\"video-78105-1\" preload=\"metadata\" width=\"1280\"><source src=\"https://blogs.nvidia.com/wp-content/uploads/2025/03/limping_panther.mp4?_=1\" type=\"video/mp4\" /><a href=\"https://blogs.nvidia.com/wp-content/uploads/2025/03/limping_panther.mp4\">https://blogs.nvidia.com/wp-content/uploads/2025/03/limping_panther.mp4</a></video></div>\n<p><i>Learn more about how NVIDIA technologies bolster conservation and environmental initiatives at </i><a href=\"https://www.nvidia.com/gtc/\" target=\"_blank\"><i>NVIDIA GTC</i></a><i>, a global AI conference running March 17-21 in San Jose, California, including at sessions on how AI is supercharging </i><a href=\"https://www.nvidia.com/gtc/session-catalog/?tab.catalogallsessionstab=16566177511100015Kus&amp;search=flora#/session/1729736872112001L1rB\" target=\"_blank\"><i>Antarctic flora monitoring</i></a><i>, enhancing a </i><a href=\"https://www.nvidia.com/gtc/session-catalog/?tab.catalogallsessionstab=16566177511100015Kus&amp;search=great%20barrier%20reef#/session/1729740910008001cGyM\" target=\"_blank\"><i>digital twin of the Great Barrier Reef</i></a><i> and helping </i><a href=\"https://www.nvidia.com/gtc/session-catalog/?tab.catalogallsessionstab=16566177511100015Kus&amp;search=pollution#/session/1729182084937001UwLo\" target=\"_blank\"><i>mitigate urban climate change</i></a><i>.</i></p>\n<p><i>Featured video courtesy of Conservation X Labs</i>.</p> ",
      "summary": "From Seattle, Washington, to Cape Town, South Africa — and everywhere around and between — AI is helping conserve the wild plants and animals that make up the intricate web of life on Earth. It’s critical work that sustains ecosystems and supports biodiversity at a time when the United Nations estimates over 1 million species\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/ai-protects-wildlife/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "date_published": "2025-03-03T14:00:43+00:00",
      "author": {
        "name": "Angie Lee"
      }
    }
  ]
}